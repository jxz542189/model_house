{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity Analysis with tf.transform, Dataflow, and BigQuery\n",
    "\n",
    "This tutorial shows how to use document similarity analysis as follows:\n",
    "\n",
    "* Raw text data is stored in GCS (or local file system)\n",
    "* Data is processed using Apache Beam (on Dataflow) + tf.transform to produce\n",
    "  * TF.IDF\n",
    "  * Text Embedding using tf.hub text_embeddings modules\n",
    "  * Bag of Words (BOW) indices (vocab ids)\n",
    "* Each doc + Embeddings iw inserted to BigQuery\n",
    "* Similarity Analysis between docs in BigQuery are performed using Cosine Similarity\n",
    "\n",
    "The dataset used is **Reuters-21578**. Details of this corpus is found in this link: http://www.daviddlewis.com/resources/testcollections/reuters21578/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/khalidsalama/Technology/python-venvs/py27-venv/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.coders as tft_coders\n",
    "\n",
    "from tensorflow_transform.beam import impl\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "\n",
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Download Reuters Corpus\n",
    "\n",
    "The **Reuters-21578** corpus can be downloaded from a publicly accessible Google Cloud Storage bucket: **gs://cloudml-textanalysis/data/reuters.tar.gz**. After downloading and unzipping the archive folder, the the corpus is found divided to **train** and **eval** directories. Each directory contains documents, one for each article, organized in sub-directories, one for each **topic**. \n",
    "\n",
    "The following is the structure of the unzipped directories:\n",
    "\n",
    "* train\n",
    "    * ...\n",
    "    * coconut **# topic**\n",
    "    * coffee\n",
    "    * ..\n",
    "    * gas **# topic**\n",
    "        * 0000218 **# document**\n",
    "        * 0000223\n",
    "        * ...\n",
    "    * gold\n",
    "        * 0000319\n",
    "        * ...\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash \n",
    "\n",
    "# gsutil cp gs://cloudml-textanalysis/data/reuters.tar.gz ./data\n",
    "# gunzip -c data/reuters.tar.gz | tar xopf -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Params:\n",
    "    pass\n",
    "\n",
    "\n",
    "Params.MODULE_URL = 'https://tfhub.dev/google/universal-sentence-encoder/1'\n",
    "#Params.MODULE_URL = 'https://tfhub.dev/google/nnlm-en-dim50/1'\n",
    "\n",
    "Params.GCP_PROJECT_ID = 'ksalama-gcp-playground'\n",
    "Params.BQ_DATASET = 'playground_ds'\n",
    "Params.BQ_TABLE = 'reuters_embeddings'\n",
    "\n",
    "Params.ROOT = '.' # 'gs://cloudml-textanalysis' | '.'\n",
    "\n",
    "Params.DATA_DIR = os.path.join(Params.ROOT, 'data/reuters')\n",
    "Params.TRANSFORMED_DATA_DIR = os.path.join(Params.DATA_DIR, 'transformed')\n",
    "\n",
    "\n",
    "Params.RAW_TRAIN_DATA_DIR = os.path.join(Params.DATA_DIR, 'train')\n",
    "Params.RAW_EVAL_DATA_DIR = os.path.join(Params.DATA_DIR, 'eval')\n",
    "\n",
    "Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX = os.path.join(Params.TRANSFORMED_DATA_DIR, 'train/docs-encoding')\n",
    "Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX = os.path.join(Params.TRANSFORMED_DATA_DIR, 'eval/docs-encoding')\n",
    "Params.TRANSFORMED_ENCODING='text'\n",
    "\n",
    "Params.TEMP_DIR = os.path.join(Params.DATA_DIR, 'tmp')\n",
    "\n",
    "Params.MODELS_DIR = os.path.join(Params.ROOT, 'models/reuters')\n",
    "\n",
    "Params.TRANSFORM_ARTEFACTS_DIR = os.path.join(Params.MODELS_DIR,'transform')\n",
    "\n",
    "Params.RUNNER = 'DirectRunner' # DirectRunner | DataflowRunner\n",
    "\n",
    "Params.TEST_MODE = True # process only few docs for testing\n",
    "\n",
    "Params.TEST_MODE_SAMPLE = 3 # number of topics & number of docs in each topic to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sink BigQuery Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigquey_schema():\n",
    "\n",
    "    from apache_beam.io.gcp.internal.clients import bigquery\n",
    "    \n",
    "    table_schema = bigquery.TableSchema()\n",
    "\n",
    "    topic_schema = bigquery.TableFieldSchema()\n",
    "    topic_schema.name = 'topic'\n",
    "    topic_schema.type = 'string'\n",
    "    topic_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(topic_schema)\n",
    "    \n",
    "    title_schema = bigquery.TableFieldSchema()\n",
    "    title_schema.name = 'title'\n",
    "    title_schema.type = 'string'\n",
    "    title_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(title_schema)\n",
    "    \n",
    "    embed_schema = bigquery.TableFieldSchema()\n",
    "    embed_schema.name = 'embeddings'\n",
    "    embed_schema.type = 'float'\n",
    "    embed_schema.mode = 'repeated'\n",
    "    table_schema.fields.append(embed_schema)\n",
    "    \n",
    "    return table_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DELIMITERS = '.,!?() '\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "def get_paths(directory):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    sub_directories = tf.gfile.ListDirectory(directory)\n",
    "    \n",
    "    if Params.TEST_MODE:\n",
    "        sub_directories = sub_directories[:Params.TEST_MODE_SAMPLE]\n",
    "        \n",
    "    return [os.path.join(directory,path) for path in sub_directories if '.DS_' not in path]\n",
    "\n",
    "def get_name_and_content(file_name):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    content = tf.gfile.GFile(file_name).read()\n",
    "    return file_name, content\n",
    "\n",
    "def get_title_and_topic((file_name, content)):\n",
    "    \n",
    "    topic = file_name.split('/')[-2]\n",
    "    title = content.split('\\r')[1].replace('\\n','')\n",
    "    return topic, title\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import nltk\n",
    "    \n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    tokenized_words = word_tokenize(text.lower())\n",
    "    tokenized_words = [''.join(c for c in word if c.isalnum()) for word in tokenized_words]\n",
    "    clean_text = ' '.join([word.strip() for word in tokenized_words if word not in stop_words and word != ''])\n",
    "    return clean_text\n",
    "\n",
    "def to_dictionary(input_tuple):\n",
    "    \n",
    "    output_dict = dict()\n",
    "    output_dict['topic'] = input_tuple[0]\n",
    "    output_dict['raw_title'] = input_tuple[1]\n",
    "    output_dict['clean_title'] = input_tuple[2]\n",
    "    return output_dict\n",
    "\n",
    "def get_raw_metadata():\n",
    "    \n",
    "    raw_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema({\n",
    "    'topic': dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n",
    "    'raw_title': dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n",
    "    'clean_title': dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n",
    "    }))\n",
    "    \n",
    "    return raw_metadata\n",
    "\n",
    "def get_embeddings(text):\n",
    "    \n",
    "    import tensorflow_hub as hub\n",
    "    embed = hub.Module(Params.MODULE_URL)\n",
    "    embeddings = embed(text)\n",
    "    return embeddings\n",
    "\n",
    "def preprocessing_fn(input_features):\n",
    "    \n",
    "    # get the text of  clean_title\n",
    "    text = input_features['clean_title']\n",
    "    \n",
    "    # extract embeddings using tf.hub\n",
    "    embeddings = tft.apply_function(get_embeddings, text)\n",
    "\n",
    "    # tokenize text\n",
    "    text_tokens = tf.string_split(text, DELIMITERS)\n",
    "    \n",
    "    # bag of words (bow) indicies\n",
    "    text_tokens_indices = tft.string_to_int(text_tokens, top_k=VOCAB_SIZE)\n",
    "    \n",
    "    # tf.idf\n",
    "    bag_of_words_indices, tf_idf = tft.tfidf(text_tokens_indices, VOCAB_SIZE + 1)\n",
    "    \n",
    "    output_features = dict()\n",
    "    output_features['topic'] = input_features['topic']\n",
    "    output_features['title'] = input_features['raw_title']\n",
    "    output_features['bow'] = bag_of_words_indices\n",
    "    output_features['tf_idf'] = tf_idf\n",
    "    output_features['embeddings'] = embeddings\n",
    "    \n",
    "    return output_features\n",
    "    \n",
    "\n",
    "def to_bq_row(entry):\n",
    "    \n",
    "    valid_embeddings = [round(float(e), 3) for e in entry['embeddings']]\n",
    "    \n",
    "    return {\n",
    "        \"topic\": entry['topic'],\n",
    "        \"title\": entry['title'],\n",
    "        \"embeddings\": valid_embeddings\n",
    "    }  \n",
    "\n",
    "\n",
    "############ Beam Pipeline Functions ####################################\n",
    "\n",
    "def read_raw_data(pipeline, source, step):\n",
    "    \n",
    "    raw_data = (\n",
    "        pipeline\n",
    "        | '{} - Get Directories'.format(step) >> beam.Create(get_paths(source))\n",
    "        | '{} - Get Files'.format(step) >> beam.FlatMap(get_paths)\n",
    "        | '{} - Read Content'.format(step) >> beam.Map(get_name_and_content)\n",
    "        | '{} - Get Title & Topic'.format(step) >> beam.Map(get_title_and_topic)\n",
    "        | '{} - Clean Title'.format(step) >> beam.Map(lambda (topic, title): (topic, title, clean_text(title)))\n",
    "        | '{} - To Dictionary'.format(step) >> beam.Map(to_dictionary)\n",
    "    )\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "def write_to_files(dataset, sink, encoding, step):\n",
    "\n",
    "    data, metadata = dataset\n",
    "    \n",
    "    if encoding == 'tfrecords':\n",
    "        (\n",
    "            data \n",
    "             | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                 file_path_prefix=sink,\n",
    "                 file_name_suffix=\".tfrecords\",\n",
    "                 coder=tft_coders.example_proto_coder.ExampleProtoCoder(metadata.schema))\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        (\n",
    "            data \n",
    "              | '{} - Write Transformed Data'.format(step) >> beam.io.textio.WriteToText(\n",
    "                  file_path_prefix=sink,\n",
    "                  file_name_suffix=\".txt\")\n",
    "        )\n",
    "        \n",
    "def write_to_bq(dataset, project_id, bq_dataset_name, bq_table_name, bq_table_schema, step):\n",
    "    \n",
    "    data, metadata = dataset\n",
    "    \n",
    "    (\n",
    "        data\n",
    "        | '{} - Convert to Valid BQ Row'.format(step) >> beam.Map(to_bq_row)\n",
    "        | '{} - Write to BigQuery'.format(step) >> beam.io.WriteToBigQuery(\n",
    "            project=project_id, dataset=bq_dataset_name, table=bq_table_name, schema=bq_table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "        )\n",
    "    )\n",
    "    \n",
    "#####################################################################################################\n",
    "\n",
    "def run_transformation_pipeline(runner, options):\n",
    "    \n",
    "    options = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    with beam.Pipeline(runner, options=options) as pipeline:\n",
    "        with impl.Context(Params.TEMP_DIR):\n",
    "            \n",
    "            ################## train data ##################\n",
    "            \n",
    "            step = 'Train'\n",
    "            \n",
    "            ### read raw train data\n",
    "            raw_train_data = read_raw_data(pipeline, Params.RAW_TRAIN_DATA_DIR, step)\n",
    "\n",
    "            ### create a dataset from the train data and schema\n",
    "            raw_train_dataset = (raw_train_data, get_raw_metadata())\n",
    "            \n",
    "            ### analyze and transform raw_train_dataset to produced transformed_dataset and transform_fn\n",
    "            transformed_train_dataset, transform_fn = (\n",
    "                raw_train_dataset \n",
    "                | 'Analyze & Transform' >> impl.AnalyzeAndTransformDataset(\n",
    "                    preprocessing_fn\n",
    "                )\n",
    "            )\n",
    "        \n",
    "            \n",
    "            ### write transformed train data to files\n",
    "            write_to_files(\n",
    "                transformed_train_dataset, \n",
    "                Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX,\n",
    "                Params.TRANSFORMED_ENCODING,\n",
    "                step, \n",
    "            )\n",
    "            \n",
    "            ## write to train embeddings to BigQuery\n",
    "            write_to_bq(\n",
    "                transformed_train_dataset,\n",
    "                Params.GCP_PROJECT_ID, \n",
    "                Params.BQ_DATASET, \n",
    "                Params.BQ_TABLE, \n",
    "                create_bigquey_schema(), \n",
    "                step\n",
    "            )\n",
    "            \n",
    "            ################## eval data ##################\n",
    "            \n",
    "            step = 'Eval'\n",
    "\n",
    "            ### read raw eval data\n",
    "            raw_eval_data = read_raw_data(pipeline, Params.RAW_EVAL_DATA_DIR, step)\n",
    "                        \n",
    "            ### create a dataset from the train data and schema\n",
    "            raw_eval_dataset = (raw_eval_data, get_raw_metadata())\n",
    "\n",
    "            # transform eval data based on produced transform_fn (from analyzing train_data)\n",
    "            transformed_eval_dataset = (\n",
    "                (raw_eval_dataset, transform_fn) \n",
    "                | '{} - Transform'.format(step) >> impl.TransformDataset()\n",
    "            )\n",
    "\n",
    "            ### write transformed eval data to files\n",
    "            write_to_files(\n",
    "                transformed_train_dataset, \n",
    "                Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX,\n",
    "                Params.TRANSFORMED_ENCODING,\n",
    "                step, \n",
    "            )\n",
    "            \n",
    "            ## write to eval embeddings to BigQuery\n",
    "            write_to_bq(\n",
    "                transformed_eval_dataset,\n",
    "                Params.GCP_PROJECT_ID, \n",
    "                Params.BQ_DATASET, \n",
    "                Params.BQ_TABLE, \n",
    "                create_bigquey_schema(), \n",
    "                step\n",
    "            )\n",
    "\n",
    "            ################## write transformation artefacts ##################\n",
    "            \n",
    "            (\n",
    "                transform_fn \n",
    "                | 'Write Transform Artefacts' >> transform_fn_io.WriteTransformFn(\n",
    "                    Params.TRANSFORM_ARTEFACTS_DIR\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "tensorflow>=1.7 \n",
    "tensorflow-transform==0.6.0 \n",
    "tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/reuters/train\n",
      "Test Mode is ON\n",
      "./data/reuters/train/tin\n",
      "-------------------------\n",
      "./data/reuters/train/tin/0007261\n",
      "./data/reuters/train/tin/0000552\n",
      "./data/reuters/train/tin/0005647\n",
      "\n",
      "./data/reuters/train/l-cattle\n",
      "-------------------------\n",
      "./data/reuters/train/l-cattle/0003173\n",
      "./data/reuters/train/l-cattle/0004107\n",
      "./data/reuters/train/l-cattle/0000316\n",
      "\n",
      "./data/reuters/train/linseed\n",
      "-------------------------\n",
      "./data/reuters/train/linseed/0000002\n",
      "./data/reuters/train/linseed/0008699\n",
      "\n",
      "Topic: 3\n",
      "Docs to process: 8\n"
     ]
    }
   ],
   "source": [
    "print 'Source data location: {}'.format(Params.RAW_TRAIN_DATA_DIR)\n",
    "\n",
    "if Params.TEST_MODE:\n",
    "    print \"Test Mode is ON\"\n",
    "    print \"\"\n",
    "    \n",
    "    doc_count = 0\n",
    "    for directory in get_paths(Params.RAW_TRAIN_DATA_DIR):\n",
    "        print directory\n",
    "        print \"-------------------------\"\n",
    "        for doc in get_paths(directory):\n",
    "            print doc\n",
    "            doc_count += 1\n",
    "        print \"\"\n",
    "        \n",
    "    print 'Topic: {}'.format(Params.TEST_MODE_SAMPLE)\n",
    "    print 'Docs to process: {}'.format(doc_count)\n",
    "    \n",
    "else:\n",
    "    'Test Mode is Off. All the docs will be processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform directories are removed...\n",
      "\n",
      "Launching DirectRunner job process-reuters-docs-180527-141215 ... hang on\n",
      "\n",
      "Transformation job started at 14:12:15\n",
      ".......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n",
      "WARNING:root:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    " \n",
    "try:\n",
    "    tf.gfile.DeleteRecursively(Params.TRANSFORM_ARTEFACTS_DIR)\n",
    "    tf.gfile.DeleteRecursively(Params.TRANSFORMED_DATA_DIR)\n",
    "    tf.gfile.DeleteRecursively(Params.TEMP_DIR)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print 'Transform directories are removed...'\n",
    "print ''\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "job_name = 'process-reuters-docs-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "print 'Launching {} job {} ... hang on'.format(Params.RUNNER, job_name)\n",
    "print(\"\")\n",
    "\n",
    "options = {\n",
    "    'region': 'europe-west1',\n",
    "    'staging_location': os.path.join(Params.TEMP_DIR,'staging'),\n",
    "    'temp_location': Params.TEMP_DIR,\n",
    "    'job_name': job_name,\n",
    "    'project': Params.GCP_PROJECT_ID,\n",
    "    'worker_machine_type': 'n1-standard-1',\n",
    "    'max_num_workers': 50,\n",
    "    'requirements_file': 'requirements.txt'\n",
    "}\n",
    "\n",
    "if Params.RUNNER == 'DirectRunner':\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Transformation job started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "\n",
    "    run_transformation_pipeline(Params.RUNNER, options)\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Transformation job finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Job elapsed time: {} \".format(time_elapsed.total_seconds()))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    run_transformation_pipeline(Params.RUNNER, options)\n",
    "    print(\"Dataflow job submitted successfully...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
