{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# pip install tensorflow==1.7\n",
    "# pip install tensorflow-transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using TensorFlow and Google Cloud - Part 4\n",
    "\n",
    "This [bigquery-public-data:hacker_news](https://cloud.google.com/bigquery/public-data/hacker-news) contains all stories and comments from Hacker News from its launch in 2006.  Each story contains a story id, url, the title of the story, tthe author that made the post, when it was written, and the number of points the story received.\n",
    "\n",
    "The objective is, given the title of the story, we want to build an ML model that can predict the source of this story.\n",
    "\n",
    "## TF DNNClassifier with TF.IDF Text Reprsentation\n",
    "\n",
    "This notebook illustrates how to build a TF premade estimator, namely DNNClassifier, while the input text will be repesented as TF.IDF computed during the preprocessing phase in Part 1. The overall steps are as follows:\n",
    "\n",
    "\n",
    "1. Define the metadata\n",
    "2. Define data input function\n",
    "2. Create feature columns (using the tfidf)\n",
    "3. Create the premade DNNClassifier estimator\n",
    "4. Setup experiement\n",
    "    * Hyper-parameters & RunConfig\n",
    "    * Serving function (for exported model)\n",
    "    * TrainSpec & EvalSpec\n",
    "5. Run experiement\n",
    "6. Evalute the model\n",
    "7. Use SavedModel for prediction\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Params:\n",
    "    pass\n",
    "\n",
    "# Set to run on GCP\n",
    "Params.GCP_PROJECT_ID = 'ksalama-gcp-playground'\n",
    "Params.REGION = 'europe-west1'\n",
    "Params.BUCKET = 'ksalama-gcs-cloudml'\n",
    "\n",
    "Params.PLATFORM = 'local' # local | GCP\n",
    "\n",
    "Params.DATA_DIR = 'data/news'  if Params.PLATFORM == 'local' else 'gs://{}/data/news'.format(Params.BUCKET)\n",
    "\n",
    "Params.TRANSFORMED_DATA_DIR = os.path.join(Params.DATA_DIR, 'transformed')\n",
    "Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX = os.path.join(Params.TRANSFORMED_DATA_DIR, 'train')\n",
    "Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX = os.path.join(Params.TRANSFORMED_DATA_DIR, 'eval')\n",
    "\n",
    "Params.TEMP_DIR = os.path.join(Params.DATA_DIR, 'tmp')\n",
    "\n",
    "Params.MODELS_DIR = 'models/news' if Params.PLATFORM == 'local' else 'gs://{}/models/news'.format(Params.BUCKET)\n",
    "\n",
    "Params.TRANSFORM_ARTEFACTS_DIR = os.path.join(Params.MODELS_DIR,'transform')\n",
    "\n",
    "Params.TRAIN = True\n",
    "\n",
    "Params.RESUME_TRAINING = False\n",
    "\n",
    "Params.EAGER = False\n",
    "\n",
    "if Params.EAGER:\n",
    "    tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/khalidsalama/Technology/python-venvs/py27-venv/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.utils import input_fn_utils\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.saved import saved_transform_io\n",
    "\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'source': FixedLenFeature(shape=[], dtype=tf.string, default_value=None), u'title': FixedLenFeature(shape=[], dtype=tf.string, default_value=None), u'weight': VarLenFeature(dtype=tf.float32), u'bow': VarLenFeature(dtype=tf.int64)}\n"
     ]
    }
   ],
   "source": [
    "RAW_HEADER = 'key,title,source'.split(',')\n",
    "RAW_DEFAULTS = [['NA'],['NA'],['NA']]\n",
    "TARGET_FEATURE_NAME = 'source'\n",
    "TARGET_LABELS = ['github', 'nytimes', 'techcrunch']\n",
    "TEXT_FEATURE_NAME = 'title'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "TRAIN_SIZE = 73124\n",
    "EVAL_SIZE = 23079\n",
    "\n",
    "DELIMITERS = '.,!?() '\n",
    "\n",
    "raw_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema({\n",
    "    KEY_COLUMN: dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n",
    "    TEXT_FEATURE_NAME: dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n",
    "    TARGET_FEATURE_NAME: dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n",
    "}))\n",
    "\n",
    "\n",
    "transformed_metadata = metadata_io.read_metadata(\n",
    "    os.path.join(Params.TRANSFORM_ARTEFACTS_DIR,\"transformed_metadata\"))\n",
    "\n",
    "raw_feature_spec = raw_metadata.schema.as_feature_spec()\n",
    "transformed_feature_spec = transformed_metadata.schema.as_feature_spec()\n",
    "\n",
    "print transformed_feature_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_example(tf_example):\n",
    "    \n",
    "    parsed_features = tf.parse_single_example(serialized=tf_example, features=transformed_feature_spec)\n",
    "    target = parsed_features.pop(TARGET_FEATURE_NAME)\n",
    "    \n",
    "    return parsed_features, target\n",
    "\n",
    "\n",
    "def generate_tfrecords_input_fn(files_pattern, \n",
    "                          mode=tf.estimator.ModeKeys.EVAL, \n",
    "                          num_epochs=1, \n",
    "                          batch_size=200):\n",
    "    \n",
    "    def _input_fn():\n",
    "        \n",
    "        file_names = data.Dataset.list_files(files_pattern)\n",
    "\n",
    "        if Params.EAGER:\n",
    "            print file_names\n",
    "\n",
    "        dataset = data.TFRecordDataset(file_names )\n",
    "\n",
    "        dataset = dataset.apply(\n",
    "                tf.contrib.data.shuffle_and_repeat(count=num_epochs,\n",
    "                                                   buffer_size=batch_size*2)\n",
    "        )\n",
    "\n",
    "        dataset = dataset.apply(\n",
    "                tf.contrib.data.map_and_batch(parse_tf_example, \n",
    "                                              batch_size=batch_size, \n",
    "                                              num_parallel_batches=2)\n",
    "        )\n",
    "\n",
    "        datset = dataset.prefetch(batch_size)\n",
    "\n",
    "        if Params.EAGER:\n",
    "            return dataset\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        features, target = iterator.get_next()\n",
    "        return features, target\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_FEATURE_NAME = 'bow'\n",
    "TFIDF_FEATURE_NAME = 'weight'\n",
    "\n",
    "def create_feature_columns():\n",
    "    \n",
    "    # Get word indecies from bow\n",
    "    bow = tf.feature_column.categorical_column_with_identity(\n",
    "      BOW_FEATURE_NAME, num_buckets=VOCAB_SIZE + 1)\n",
    "    \n",
    "    # Add weight to the word indecies\n",
    "    weight_bow = tf.feature_column.weighted_categorical_column(\n",
    "      bow, TFIDF_FEATURE_NAME)\n",
    "    \n",
    "    # Convert to indicator \n",
    "    weight_bow_indicators = tf.feature_column.indicator_column(weight_bow)\n",
    "    \n",
    "    return [weight_bow_indicators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a model using a premade DNNClassifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(hparams, run_config):\n",
    "    \n",
    "    feature_columns = create_feature_columns()\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)\n",
    "    \n",
    "    estimator = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        n_classes =len(TARGET_LABELS),\n",
    "        label_vocabulary=TARGET_LABELS,\n",
    "        hidden_units=hparams.hidden_units,\n",
    "        optimizer=optimizer,\n",
    "        config=run_config\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 HParams and RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batch_size', 1000), ('hidden_units', [64, 32]), ('learning_rate', 0.01), ('max_steps', 730), ('num_epochs', 10), ('trainable_embedding', False)]\n",
      "\n",
      "('Model Directory:', 'models/news/dnn_estimator_tfidf')\n",
      "('Dataset Size:', 73124)\n",
      "('Batch Size:', 1000)\n",
      "('Steps per Epoch:', 73)\n",
      "('Total Steps:', 730)\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "TOTAL_STEPS = (TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS\n",
    "EVAL_EVERY_SEC = 60\n",
    "\n",
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    learning_rate = 0.01,\n",
    "    hidden_units=[64, 32],\n",
    "    max_steps = TOTAL_STEPS,\n",
    "\n",
    ")\n",
    "\n",
    "MODEL_NAME = 'dnn_estimator_tfidf' \n",
    "model_dir = os.path.join(Params.MODELS_DIR, MODEL_NAME)\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    log_step_count_steps=1000,\n",
    "    save_checkpoints_secs=EVAL_EVERY_SEC,\n",
    "    keep_checkpoint_max=1,\n",
    "    model_dir=model_dir\n",
    ")\n",
    "\n",
    "\n",
    "print(hparams)\n",
    "print(\"\")\n",
    "print(\"Model Directory:\", run_config.model_dir)\n",
    "print(\"Dataset Size:\", TRAIN_SIZE)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Steps per Epoch:\",TRAIN_SIZE/BATCH_SIZE)\n",
    "print(\"Total Steps:\", TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_serving_input_fn():\n",
    "    \n",
    "    def _serving_fn():\n",
    "    \n",
    "        receiver_tensor = {\n",
    "          'title': tf.placeholder(dtype=tf.string, shape=[None])\n",
    "        }\n",
    "\n",
    "        _, transformed_features = (\n",
    "            saved_transform_io.partially_apply_saved_transform(\n",
    "                os.path.join(Params.TRANSFORM_ARTEFACTS_DIR, transform_fn_io.TRANSFORM_FN_DIR),\n",
    "            receiver_tensor)\n",
    "        )\n",
    "        \n",
    "        return tf.estimator.export.ServingInputReceiver(\n",
    "            transformed_features, receiver_tensor)\n",
    "    \n",
    "    return _serving_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 TrainSpec & EvalSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn = generate_tfrecords_input_fn(\n",
    "        Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX+\"*\",\n",
    "        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "        num_epochs=hparams.num_epochs,\n",
    "        batch_size=hparams.batch_size\n",
    "    ),\n",
    "    max_steps=hparams.max_steps,\n",
    "    hooks=None\n",
    ")\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = generate_tfrecords_input_fn(\n",
    "        Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX+\"*\",\n",
    "        mode=tf.estimator.ModeKeys.EVAL,\n",
    "        num_epochs=1,\n",
    "        batch_size=hparams.batch_size\n",
    "    ),\n",
    "    exporters=[tf.estimator.LatestExporter(\n",
    "        name=\"estimate\", # the name of the folder in which the model will be exported to under export\n",
    "        serving_input_receiver_fn=generate_serving_input_fn(),\n",
    "        exports_to_keep=1,\n",
    "        as_text=False)],\n",
    "    steps=None,\n",
    "    throttle_secs=EVAL_EVERY_SEC\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previous training artefacts...\n",
      "Experiment started at 16:13:21\n",
      ".......................................\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 60, '_session_config': None, '_keep_checkpoint_max': 1, '_tf_random_seed': 19830610, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11711e1d0>, '_model_dir': 'models/news/dnn_estimator_tfidf', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 1000, '_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 60 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into models/news/dnn_estimator_tfidf/model.ckpt.\n",
      "INFO:tensorflow:loss = 1098.7266, step = 1\n",
      "INFO:tensorflow:loss = 213.40088, step = 101 (15.307 sec)\n",
      "INFO:tensorflow:loss = 147.65674, step = 201 (13.971 sec)\n",
      "INFO:tensorflow:loss = 71.7646, step = 301 (15.121 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 392 into models/news/dnn_estimator_tfidf/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 26.048763.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-14-16:14:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/news/dnn_estimator_tfidf/model.ckpt-392\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-14-16:14:25\n",
      "INFO:tensorflow:Saving dict for global step 392: accuracy = 0.8243858, average_loss = 0.94847244, global_step = 392, loss = 912.07477\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\t\\n\\007Const:0\\022\\033vocab_string_to_int_uniques\"\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Restoring parameters from models/news/dnn_estimator_tfidf/model.ckpt-392\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: models/news/dnn_estimator_tfidf/export/estimate/temp-1526314465/assets\n",
      "INFO:tensorflow:SavedModel written to: models/news/dnn_estimator_tfidf/export/estimate/temp-1526314465/saved_model.pb\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/news/dnn_estimator_tfidf/model.ckpt-392\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 393 into models/news/dnn_estimator_tfidf/model.ckpt.\n",
      "INFO:tensorflow:loss = 27.088547, step = 393\n",
      "INFO:tensorflow:loss = 2.9095829, step = 493 (13.979 sec)\n",
      "INFO:tensorflow:loss = 4.3351374, step = 593 (13.651 sec)\n",
      "INFO:tensorflow:loss = 11.017786, step = 693 (14.415 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 730 into models/news/dnn_estimator_tfidf/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.2552278.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-14-16:15:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from models/news/dnn_estimator_tfidf/model.ckpt-730\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-14-16:15:17\n",
      "INFO:tensorflow:Saving dict for global step 730: accuracy = 0.82416916, average_loss = 1.344607, global_step = 730, loss = 1293.0077\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\t\\n\\007Const:0\\022\\033vocab_string_to_int_uniques\"\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Restoring parameters from models/news/dnn_estimator_tfidf/model.ckpt-730\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: models/news/dnn_estimator_tfidf/export/estimate/temp-1526314518/assets\n",
      "INFO:tensorflow:SavedModel written to: models/news/dnn_estimator_tfidf/export/estimate/temp-1526314518/saved_model.pb\n",
      ".......................................\n",
      "Experiment finished at 16:15:18\n",
      "\n",
      "Experiment elapsed time: 117.021302 seconds\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "if Params.TRAIN:\n",
    "    if not Params.RESUME_TRAINING:\n",
    "        print(\"Removing previous training artefacts...\")\n",
    "        shutil.rmtree(model_dir, ignore_errors=True)\n",
    "    else:\n",
    "        print(\"Resuming training...\") \n",
    "\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "\n",
    "    estimator = create_estimator(hparams, run_config)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec, \n",
    "        eval_spec=eval_spec\n",
    "    )\n",
    "\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "else:\n",
    "    print \"Training was skipped!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################################################\n",
      "# Train Measures: {'average_loss': 0.0037224626, 'accuracy': 0.99904275, 'global_step': 730, 'loss': 272.20135}\n",
      "############################################################################################\n",
      "\n",
      "############################################################################################\n",
      "# Eval Measures: {'average_loss': 1.3446056, 'accuracy': 0.82416916, 'global_step': 730, 'loss': 31032.152}\n",
      "############################################################################################\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "estimator = create_estimator(hparams, run_config)\n",
    "\n",
    "train_metrics = estimator.evaluate(\n",
    "    input_fn = generate_tfrecords_input_fn(\n",
    "        files_pattern= Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX+\"*\", \n",
    "        mode= tf.estimator.ModeKeys.EVAL,\n",
    "        batch_size= TRAIN_SIZE), \n",
    "    steps=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Train Measures: {}\".format(train_metrics))\n",
    "print(\"############################################################################################\")\n",
    "\n",
    "eval_metrics = estimator.evaluate(\n",
    "    input_fn=generate_tfrecords_input_fn(\n",
    "        files_pattern= Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX+\"*\", \n",
    "        mode= tf.estimator.ModeKeys.EVAL,\n",
    "        batch_size= EVAL_SIZE), \n",
    "    steps=1\n",
    ")\n",
    "print(\"\")\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Eval Measures: {}\".format(eval_metrics))\n",
    "print(\"############################################################################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Use Saved Model for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/news/dnn_estimator_tfidf/export/estimate/1526314518\n",
      "\n",
      "{u'probabilities': array([[0.96217114, 0.01375495, 0.02407398],\n",
      "       [0.02322701, 0.39720485, 0.5795681 ],\n",
      "       [0.03017025, 0.9552083 , 0.01462139]], dtype=float32), u'class_ids': array([[0],\n",
      "       [2],\n",
      "       [1]]), u'classes': array([['github'],\n",
      "       ['techcrunch'],\n",
      "       ['nytimes']], dtype=object), u'logits': array([[ 2.4457023, -1.8020908, -1.2423583],\n",
      "       [-2.1229138,  0.7162221,  1.0940531],\n",
      "       [-0.9709409,  2.4841323, -1.6953117]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "export_dir = model_dir +\"/export/estimate/\"\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[0])\n",
    "\n",
    "print(saved_model_dir)\n",
    "print(\"\")\n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "    export_dir = saved_model_dir,\n",
    "    signature_def_key=\"predict\"\n",
    ")\n",
    "\n",
    "output = predictor_fn(\n",
    "    {\n",
    "        'title':[\n",
    "            'Microsoft and Google are joining forces for a new AI framework',\n",
    "            'A new version of Python is mind blowing',\n",
    "            'EU is investigating new data privacy policies'\n",
    "        ]\n",
    "        \n",
    "    }\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
