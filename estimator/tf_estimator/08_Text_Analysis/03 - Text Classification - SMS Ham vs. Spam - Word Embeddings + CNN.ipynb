{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI SMS Spam Collection Dataset\n",
    "\n",
    "* **Input**: sms textual content. **Target**: ham or spam\n",
    "* **data representation**: each sms is repesented with a **fixed-length vector of word indexes**. A word index lookup is generated from the vocabulary list.\n",
    "* **words embedding**: A word embedding (dense vector) is learnt for each word. That is, each sms is presented as a matrix of (document-word-count, word-embedding-size)\n",
    "* **convolution layer**: Apply filter(s) to the word-embedding matrix, before input to the fully-connected NN\n",
    "* **train-data.tsv, valid-datat.tsv**, and **vocab_list.tsv** are prepared and saved in 'data/sms-spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalidsalama/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "import shutil\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'sms-class-model-01'\n",
    "\n",
    "TRAIN_DATA_FILES_PATTERN = 'data/sms-spam/train-*.tsv'\n",
    "VALID_DATA_FILES_PATTERN = 'data/sms-spam/valid-*.tsv'\n",
    "\n",
    "VOCAB_LIST_FILE = 'data/sms-spam/vocab_list.tsv'\n",
    "N_WORDS_FILE = 'data/sms-spam/n_words.tsv'\n",
    "\n",
    "RESUME_TRAINING = False\n",
    "MULTI_THREADING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11332\n"
     ]
    }
   ],
   "source": [
    "MAX_DOCUMENT_LENGTH = 100\n",
    "\n",
    "PAD_WORD = '#=KS=#'\n",
    "\n",
    "HEADER = ['class', 'sms']\n",
    "HEADER_DEFAULTS = [['NA'], ['NA']]\n",
    "\n",
    "TEXT_FEATURE_NAME = 'sms'\n",
    "\n",
    "TARGET_NAME = 'class'\n",
    "\n",
    "WEIGHT_COLUNM_NAME = 'weight'\n",
    "\n",
    "TARGET_LABELS = ['spam', 'ham']\n",
    "\n",
    "with open(N_WORDS_FILE) as file:\n",
    "    N_WORDS = int(file.read())+2\n",
    "print(N_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Data Input Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. TSV parsing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_tsv_row(tsv_row):\n",
    "    \n",
    "    columns = tf.decode_csv(tsv_row, record_defaults=HEADER_DEFAULTS, field_delim='\\t')\n",
    "    features = dict(zip(HEADER, columns))\n",
    "    \n",
    "    target = features.pop(TARGET_NAME)\n",
    "    \n",
    "    # giving more weight to \"spam\" records are the are only 13% of the training set\n",
    "    features[WEIGHT_COLUNM_NAME] =  tf.cond( tf.equal(target,'spam'), lambda: 6.6, lambda: 1.0 ) \n",
    "\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Data pipeline input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_label_column(label_string_tensor):\n",
    "    table = tf.contrib.lookup.index_table_from_tensor(tf.constant(TARGET_LABELS))\n",
    "    return table.lookup(label_string_tensor)\n",
    "\n",
    "def input_fn(files_name_pattern, mode=tf.estimator.ModeKeys.EVAL, \n",
    "                 skip_header_lines=0, \n",
    "                 num_epochs=1, \n",
    "                 batch_size=200):\n",
    "    \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    num_threads = multiprocessing.cpu_count() if MULTI_THREADING else 1\n",
    "    \n",
    "    buffer_size = 2 * batch_size + 1\n",
    "   \n",
    "    print(\"\")\n",
    "    print(\"* data input_fn:\")\n",
    "    print(\"================\")\n",
    "    print(\"Input file(s): {}\".format(files_name_pattern))\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    print(\"Epoch Count: {}\".format(num_epochs))\n",
    "    print(\"Mode: {}\".format(mode))\n",
    "    print(\"Thread Count: {}\".format(num_threads))\n",
    "    print(\"Shuffle: {}\".format(shuffle))\n",
    "    print(\"================\")\n",
    "    print(\"\")\n",
    "\n",
    "    file_names = tf.matching_files(files_name_pattern)\n",
    "    dataset = data.TextLineDataset(filenames=file_names)\n",
    "    \n",
    "    dataset = dataset.skip(skip_header_lines)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "        \n",
    "    dataset = dataset.map(lambda tsv_row: parse_tsv_row(tsv_row), \n",
    "                          num_parallel_calls=num_threads)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.prefetch(buffer_size)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    features, target = iterator.get_next()\n",
    "    return features, parse_label_column(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(text_feature):\n",
    "    \n",
    "    # Load vocabolary lookup table to map word => word_id\n",
    "    vocab_table = tf.contrib.lookup.index_table_from_file(vocabulary_file=VOCAB_LIST_FILE, \n",
    "                                                          num_oov_buckets=1, default_value=-1)\n",
    "    # Get text feature\n",
    "    smss = text_feature\n",
    "    # Split text to words -> this will produce sparse tensor with variable-lengthes (word count) entries\n",
    "    words = tf.string_split(smss)\n",
    "    # Convert sparse tensor to dense tensor by padding each entry to match the longest in the batch\n",
    "    dense_words = tf.sparse_tensor_to_dense(words, default_value=PAD_WORD)\n",
    "    # Convert word to word_ids via the vocab lookup table\n",
    "    word_ids = vocab_table.lookup(dense_words)\n",
    "    # Create a word_ids padding\n",
    "    padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n",
    "    # Pad all the word_ids entries to the maximum document length\n",
    "    word_ids_padded = tf.pad(word_ids, padding)\n",
    "    word_id_vector = tf.slice(word_ids_padded, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n",
    "    \n",
    "    # Return the final word_id_vector\n",
    "    return word_id_vector\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    hidden_units = params.hidden_units\n",
    "    output_layer_size = len(TARGET_LABELS)\n",
    "    embedding_size = params.embedding_size\n",
    "    window_size = params.window_size\n",
    "    stride = int(window_size/2)\n",
    "    filters = params.filters\n",
    "    \n",
    "    # word_id_vector\n",
    "    word_id_vector = process_text(features[TEXT_FEATURE_NAME]) \n",
    "    # print(\"word_id_vector: {}\".format(word_id_vector)) # (?, MAX_DOCUMENT_LENGTH)\n",
    "    \n",
    "    # layer to take each word_id and convert it into vector (embeddings) \n",
    "    word_embeddings = tf.contrib.layers.embed_sequence(word_id_vector, vocab_size=N_WORDS, \n",
    "                                                 embed_dim=embedding_size) \n",
    "    #print(\"word_embeddings: {}\".format(word_embeddings)) # (?, MAX_DOCUMENT_LENGTH, embbeding_size)\n",
    "    \n",
    "    # convolution\n",
    "    words_conv = tf.layers.conv1d(word_embeddings, filters=filters, kernel_size=window_size, \n",
    "                                  strides=stride, padding='SAME', activation=tf.nn.relu)\n",
    "    \n",
    "    #print(\"words_conv: {}\".format(words_conv)) # (?, MAX_DOCUMENT_LENGTH/stride, filters)\n",
    "    \n",
    "    words_conv_shape = words_conv.get_shape()\n",
    "    dim = words_conv_shape[1] * words_conv_shape[2]\n",
    "    input_layer = tf.reshape(words_conv,[-1, dim])\n",
    "    #print(\"input_layer: {}\".format(input_layer)) # (?, (MAX_DOCUMENT_LENGTH/stride)*filters)\n",
    "    \n",
    "    if hidden_units is not None:\n",
    "        \n",
    "        # Create a fully-connected layer-stack based on the hidden_units in the params\n",
    "        hidden_layers = tf.contrib.layers.stack(inputs=input_layer,\n",
    "                                                layer=tf.contrib.layers.fully_connected,\n",
    "                                                stack_args= hidden_units,\n",
    "                                                activation_fn=tf.nn.relu)\n",
    "        # print(\"hidden_layers: {}\".format(hidden_layers)) # (?, last-hidden-layer-size)\n",
    "    \n",
    "    else:\n",
    "        hidden_layers = input_layer\n",
    "\n",
    "    # Connect the output layer (logits) to the hidden layer (no activation fn)\n",
    "    logits = tf.layers.dense(inputs=hidden_layers, \n",
    "                             units=output_layer_size, \n",
    "                             activation=None)\n",
    "    \n",
    "    # print(\"logits: {}\".format(logits)) # (?, output_layer_size)\n",
    "\n",
    "    # Provide an estimator spec for `ModeKeys.PREDICT`.\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        predicted_indices = tf.argmax(probabilities, 1)\n",
    "\n",
    "        # Convert predicted_indices back into strings\n",
    "        predictions = {\n",
    "            'class': tf.gather(TARGET_LABELS, predicted_indices),\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "        export_outputs = {\n",
    "            'prediction': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        \n",
    "        # Provide an estimator spec for `ModeKeys.PREDICT` modes.\n",
    "        return tf.estimator.EstimatorSpec(mode,\n",
    "                                          predictions=predictions,\n",
    "                                          export_outputs=export_outputs)\n",
    "    \n",
    "    # weights\n",
    "    weights = features[WEIGHT_COLUNM_NAME]\n",
    "\n",
    "    # Calculate loss using softmax cross entropy\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "        logits=logits, labels=labels, \n",
    "        weights=weights\n",
    "    )\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # Create Optimiser\n",
    "        optimizer = tf.train.AdamOptimizer(params.learning_rate)\n",
    "\n",
    "        # Create training operation\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Provide an estimator spec for `ModeKeys.TRAIN` modes.\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          loss=loss, \n",
    "                                          train_op=train_op)\n",
    "        \n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        predicted_indices = tf.argmax(probabilities, 1)\n",
    "\n",
    "        # Return accuracy and area under ROC curve metrics\n",
    "        labels_one_hot = tf.one_hot(\n",
    "            labels,\n",
    "            depth=len(TARGET_LABELS),\n",
    "            on_value=True,\n",
    "            off_value=False,\n",
    "            dtype=tf.bool\n",
    "        )\n",
    "        \n",
    "        eval_metric_ops = {\n",
    "            'accuracy': tf.metrics.accuracy(labels, predicted_indices, weights=weights),\n",
    "            'auroc': tf.metrics.auc(labels_one_hot, probabilities, weights=weights)\n",
    "        }\n",
    "        \n",
    "        # Provide an estimator spec for `ModeKeys.EVAL` modes.\n",
    "        return tf.estimator.EstimatorSpec(mode, \n",
    "                                          loss=loss, \n",
    "                                          eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "def create_estimator(run_config, hparams):\n",
    "    estimator = tf.estimator.Estimator(model_fn=model_fn, \n",
    "                                  params=hparams, \n",
    "                                  config=run_config)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Estimator Type: {}\".format(type(estimator)))\n",
    "    print(\"\")\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Set HParam and RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batch_size', 250), ('embedding_size', 3), ('filters', 2), ('hidden_units', None), ('learning_rate', 0.01), ('max_steps', 167), ('num_epochs', 10), ('window_size', 3)]\n",
      "Model Directory: trained_models/sms-class-model-01\n",
      "\n",
      "Dataset Size: 4179\n",
      "Batch Size: 250\n",
      "Steps per Epoch: 16.716\n",
      "Total Steps: 167\n",
      "That is 1 evaluation step after each 60 training seconds\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = 4179\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 250\n",
    "EVAL_AFTER_SEC = 60\n",
    "TOTAL_STEPS = int((TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS)\n",
    "\n",
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    embedding_size = 3,\n",
    "    window_size = 3,\n",
    "    filters = 2,\n",
    "    hidden_units=None, #[8],\n",
    "    max_steps = TOTAL_STEPS,\n",
    "    learning_rate = 0.01\n",
    ")\n",
    "\n",
    "model_dir = 'trained_models/{}'.format(MODEL_NAME)\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    log_step_count_steps=5000,\n",
    "    tf_random_seed=19830610,\n",
    "    model_dir=model_dir\n",
    ")\n",
    "\n",
    "print(hparams)\n",
    "print(\"Model Directory:\", run_config.model_dir)\n",
    "print(\"\")\n",
    "print(\"Dataset Size:\", TRAIN_SIZE)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Steps per Epoch:\",TRAIN_SIZE/BATCH_SIZE)\n",
    "print(\"Total Steps:\", TOTAL_STEPS)\n",
    "print(\"That is 1 evaluation step after each\",EVAL_AFTER_SEC,\"training seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Define serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    \n",
    "    receiver_tensor = {\n",
    "      'sms': tf.placeholder(tf.string, [None]),\n",
    "    }\n",
    "    \n",
    "    features = {\n",
    "      key: tensor\n",
    "      for key, tensor in receiver_tensor.items()\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Define TrainSpec and EvaluSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn = lambda: input_fn(\n",
    "        TRAIN_DATA_FILES_PATTERN,\n",
    "        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "        num_epochs=hparams.num_epochs,\n",
    "        batch_size=hparams.batch_size\n",
    "    ),\n",
    "    max_steps=hparams.max_steps,\n",
    "    hooks=None\n",
    ")\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = lambda: input_fn(\n",
    "        VALID_DATA_FILES_PATTERN,\n",
    "        mode=tf.estimator.ModeKeys.EVAL,\n",
    "        batch_size=hparams.batch_size\n",
    "    ),\n",
    "    exporters=[tf.estimator.LatestExporter(\n",
    "        name=\"predict\", # the name of the folder in which the model will be exported to under export\n",
    "        serving_input_receiver_fn=serving_input_fn,\n",
    "        exports_to_keep=1,\n",
    "        as_text=True)],\n",
    "    steps=None,\n",
    "    throttle_secs = EVAL_AFTER_SEC\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Run Experiment via train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previous artifacts...\n",
      "Experiment started at 16:22:44\n",
      ".......................................\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_models/sms-class-model-01', '_tf_random_seed': 19830610, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 5000, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11401f908>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "Estimator Type: <class 'tensorflow.python.estimator.estimator.Estimator'>\n",
      "\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 60 secs (eval_spec.throttle_secs) or training is finished.\n",
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data/sms-spam/train-*.tsv\n",
      "Batch size: 250\n",
      "Epoch Count: 10\n",
      "Mode: train\n",
      "Thread Count: 4\n",
      "Shuffle: True\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into trained_models/sms-class-model-01/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.11302, step = 1\n",
      "INFO:tensorflow:loss = 0.00906904, step = 101 (0.864 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 167 into trained_models/sms-class-model-01/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00403102.\n",
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data/sms-spam/valid-*.tsv\n",
      "Batch size: 250\n",
      "Epoch Count: 1\n",
      "Mode: eval\n",
      "Thread Count: 4\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-26-16:22:50\n",
      "INFO:tensorflow:Restoring parameters from trained_models/sms-class-model-01/model.ckpt-167\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-26-16:22:51\n",
      "INFO:tensorflow:Saving dict for global step 167: accuracy = 0.958675, auroc = 0.990997, global_step = 167, loss = 0.189101\n",
      "INFO:tensorflow:Restoring parameters from trained_models/sms-class-model-01/model.ckpt-167\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: b\"trained_models/sms-class-model-01/export/predict/temp-b'1514305372'/assets\"\n",
      "INFO:tensorflow:SavedModel written to: b\"trained_models/sms-class-model-01/export/predict/temp-b'1514305372'/saved_model.pbtxt\"\n",
      ".......................................\n",
      "Experiment finished at 16:22:52\n",
      "\n",
      "Experiment elapsed time: 7.655679 seconds\n"
     ]
    }
   ],
   "source": [
    "if not RESUME_TRAINING:\n",
    "    print(\"Removing previous artifacts...\")\n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "else:\n",
    "    print(\"Resuming training...\") \n",
    "\n",
    "    \n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow() \n",
    "print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\") \n",
    "\n",
    "estimator = create_estimator(run_config, hparams)\n",
    "\n",
    "tf.estimator.train_and_evaluate(\n",
    "    estimator=estimator,\n",
    "    train_spec=train_spec, \n",
    "    eval_spec=eval_spec\n",
    ")\n",
    "\n",
    "time_end = datetime.utcnow() \n",
    "print(\".......................................\")\n",
    "print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_models/sms-class-model-01', '_tf_random_seed': 19830610, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 5000, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11401f908>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "Estimator Type: <class 'tensorflow.python.estimator.estimator.Estimator'>\n",
      "\n",
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data/sms-spam/train-*.tsv\n",
      "Batch size: 4179\n",
      "Epoch Count: 1\n",
      "Mode: eval\n",
      "Thread Count: 4\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-26-16:22:53\n",
      "INFO:tensorflow:Restoring parameters from trained_models/sms-class-model-01/model.ckpt-167\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-26-16:22:53\n",
      "INFO:tensorflow:Saving dict for global step 167: accuracy = 1.0, auroc = 1.0, global_step = 167, loss = 0.00494254\n",
      "\n",
      "######################################################################################\n",
      "# Train Measures: {'accuracy': 1.0, 'auroc': 1.0000001, 'loss': 0.0049425424, 'global_step': 167}\n",
      "######################################################################################\n",
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data/sms-spam/valid-*.tsv\n",
      "Batch size: 1393\n",
      "Epoch Count: 1\n",
      "Mode: eval\n",
      "Thread Count: 4\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-26-16:22:54\n",
      "INFO:tensorflow:Restoring parameters from trained_models/sms-class-model-01/model.ckpt-167\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-26-16:22:55\n",
      "INFO:tensorflow:Saving dict for global step 167: accuracy = 0.958675, auroc = 0.990997, global_step = 167, loss = 0.199238\n",
      "\n",
      "######################################################################################\n",
      "# Test Measures: {'accuracy': 0.95867485, 'auroc': 0.99099678, 'loss': 0.19923805, 'global_step': 167}\n",
      "######################################################################################\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = 4179\n",
    "TEST_SIZE = 1393\n",
    "\n",
    "train_input_fn = lambda: input_fn(files_name_pattern= TRAIN_DATA_FILES_PATTERN, \n",
    "                                      mode= tf.estimator.ModeKeys.EVAL,\n",
    "                                      batch_size= TRAIN_SIZE)\n",
    "\n",
    "test_input_fn = lambda: input_fn(files_name_pattern= VALID_DATA_FILES_PATTERN, \n",
    "                                      mode= tf.estimator.ModeKeys.EVAL,\n",
    "                                      batch_size= TEST_SIZE)\n",
    "\n",
    "estimator = create_estimator(run_config, hparams)\n",
    "\n",
    "train_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n",
    "print()\n",
    "print(\"######################################################################################\")\n",
    "print(\"# Train Measures: {}\".format(train_results))\n",
    "print(\"######################################################################################\")\n",
    "\n",
    "test_results = estimator.evaluate(input_fn=test_input_fn, steps=1)\n",
    "print()\n",
    "print(\"######################################################################################\")\n",
    "print(\"# Test Measures: {}\".format(test_results))\n",
    "print(\"######################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predict Using Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/sms-class-model-01/export/predict//1514305372\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from b'trained_models/sms-class-model-01/export/predict//1514305372/variables/variables'\n",
      "{'class': array([b'ham', b'spam', b'spam'], dtype=object), 'probabilities': array([[  7.56433234e-04,   9.99243617e-01],\n",
      "       [  8.26096475e-01,   1.73903525e-01],\n",
      "       [  9.40912008e-01,   5.90880252e-02]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "export_dir = model_dir +\"/export/predict/\"\n",
    "\n",
    "saved_model_dir = export_dir + \"/\" + os.listdir(path=export_dir)[-1] \n",
    "\n",
    "print(saved_model_dir)\n",
    "print(\"\")\n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "    export_dir = saved_model_dir,\n",
    "    signature_def_key=\"prediction\"\n",
    ")\n",
    "\n",
    "output = predictor_fn(\n",
    "    {\n",
    "        'sms':[\n",
    "            'ok, I will be with you in 5 min. see you then',\n",
    "            'win 1000 cash free of charge promo hot deal sexy',\n",
    "            'hot girls sexy tonight call girls waiting call chat'\n",
    "        ]\n",
    "        \n",
    "    }\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
