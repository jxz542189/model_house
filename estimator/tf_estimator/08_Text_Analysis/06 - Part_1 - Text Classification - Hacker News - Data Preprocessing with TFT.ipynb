{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# pip install tensorflow==1.7\n",
    "# pip install google-cloud-dataflow==2.3\n",
    "# pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using TensorFlow and Google Cloud - Part 1\n",
    "\n",
    "This [bigquery-public-data:hacker_news](https://cloud.google.com/bigquery/public-data/hacker-news) contains all stories and comments from Hacker News from its launch in 2006.  Each story contains a story id, url, the title of the story, tthe author that made the post, when it was written, and the number of points the story received.\n",
    "\n",
    "The objective is, given the title of the story, we want to build an ML model that can predict the source of this story.\n",
    "\n",
    "## Data preparation with tf.Transform and DataFlow\n",
    "\n",
    "This notebook illustrates how to build a Beam pipeline using tf.transform to prepare ML 'train' and 'eval' datasets. \n",
    "The pipeline includes the following steps:\n",
    "1. Read data from BigQuery\n",
    "2. Extract and clean features from BQ rows\n",
    "3. Use tf.transfrom to process the text and produce the following features for each entry\n",
    " * title: Raw text - string\n",
    " * bow: Bag of word indecies - sparse vector of integers\n",
    " * weight: TF.IDF values - sparse vector of floats\n",
    " * source: target feature - string\n",
    "4. Save the data as .tfrecord files\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Params:\n",
    "    pass\n",
    "\n",
    "# Set to run on GCP\n",
    "Params.GCP_PROJECT_ID = 'ksalama-gcp-playground'\n",
    "Params.REGION = 'europe-west1'\n",
    "Params.BUCKET = 'ksalama-gcs-cloudml'\n",
    "\n",
    "Params.PLATFORM = 'local' # local | GCP\n",
    "\n",
    "Params.DATA_DIR = 'data/news'  if Params.PLATFORM == 'local' else 'gs://{}/data/news'.format(Params.BUCKET)\n",
    "\n",
    "Params.TRANSFORMED_DATA_DIR = os.path.join(Params.DATA_DIR, 'transformed')\n",
    "Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX = os.path.join(Params.TRANSFORMED_DATA_DIR, 'train')\n",
    "Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX = os.path.join(Params.TRANSFORMED_DATA_DIR, 'eval')\n",
    "\n",
    "Params.TEMP_DIR = os.path.join(Params.DATA_DIR, 'tmp')\n",
    "\n",
    "Params.MODELS_DIR = 'models/news' if Params.PLATFORM == 'local' else 'gs://{}/models/news'.format(Params.BUCKET)\n",
    "\n",
    "Params.TRANSFORM_ARTEFACTS_DIR = os.path.join(Params.MODELS_DIR,'transform')\n",
    "\n",
    "Params.TRANSFORM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/khalidsalama/Technology/python-venvs/py27-venv/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.coders as tft_coders\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.utils import input_fn_utils\n",
    "\n",
    "from tensorflow_transform.beam import impl\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.saved import saved_transform_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Source Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_query = '''\n",
    "SELECT\n",
    "    key,\n",
    "    REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ') AS title, \n",
    "    source\n",
    "FROM\n",
    "(\n",
    "    SELECT\n",
    "        ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "        title,\n",
    "        ABS(FARM_FINGERPRINT(title)) AS Key\n",
    "    FROM\n",
    "      `bigquery-public-data.hacker_news.stories`\n",
    "    WHERE\n",
    "      REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "      AND LENGTH(title) > 10\n",
    ")\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
    "'''\n",
    "\n",
    "def get_source_query(step):\n",
    "    \n",
    "    if step == 'train':\n",
    "        source_query = 'SELECT * FROM ({}) WHERE MOD(key,100) <= 75'.format(bq_query)\n",
    "    else:\n",
    "        source_query = 'SELECT * FROM ({}) WHERE MOD(key,100) > 75'.format(bq_query)\n",
    "        \n",
    "    return source_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raw metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_HEADER = 'key,title,source'.split(',')\n",
    "RAW_DEFAULTS = [['NA'],['NA'],['NA']]\n",
    "TARGET_FEATURE_NAME = 'source'\n",
    "TARGET_LABELS = ['github', 'nytimes', 'techcrunch']\n",
    "TEXT_FEATURE_NAME = 'title'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "TRAIN_SIZE = 73124\n",
    "EVAL_SIZE = 23079\n",
    "\n",
    "DELIMITERS = '.,!?() '\n",
    "\n",
    "raw_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema({\n",
    "    KEY_COLUMN: dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n",
    "    TEXT_FEATURE_NAME: dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n",
    "    TARGET_FEATURE_NAME: dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(bq_row):\n",
    "    \n",
    "    CSV_HEADER = 'key,title,source'.split(',')\n",
    "    \n",
    "    input_features = {}\n",
    "    \n",
    "    for feature_name in CSV_HEADER:\n",
    "        input_features[feature_name] = str(bq_row[feature_name]).lower()\n",
    "        \n",
    "    return input_features\n",
    "\n",
    "\n",
    "def preprocessing_fn(input_features):\n",
    " \n",
    "    text = input_features[TEXT_FEATURE_NAME]\n",
    "\n",
    "    text_tokens = tf.string_split(text, DELIMITERS)\n",
    "    text_tokens_indcies = tft.string_to_int(text_tokens, top_k=VOCAB_SIZE)\n",
    "    bag_of_words_indices, text_weight = tft.tfidf(text_tokens_indcies, VOCAB_SIZE + 1)\n",
    "    \n",
    "    output_features = {}\n",
    "    output_features[TEXT_FEATURE_NAME] = input_features[TEXT_FEATURE_NAME]\n",
    "    output_features['bow'] = bag_of_words_indices\n",
    "    output_features['weight'] = text_weight\n",
    "    output_features[TARGET_FEATURE_NAME] = input_features[TARGET_FEATURE_NAME]\n",
    "    \n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Beam Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "\n",
    "def run_pipeline(runner, opts):\n",
    "    \n",
    "    print(\"Sink train data files: {}\".format(Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX))\n",
    "    print(\"Sink data files: {}\".format(Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX))\n",
    "    print(\"Temporary directory: {}\".format(Params.TEMP_DIR))\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    with beam.Pipeline(runner, options=opts) as pipeline:\n",
    "        with impl.Context(Params.TEMP_DIR): \n",
    "    \n",
    "            ###### analyze & transform train #########################################################\n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"\")\n",
    "                print(\"Transform training data....\")\n",
    "                print(\"\")\n",
    "            \n",
    "            step = 'train'\n",
    "            source_query = get_source_query(step)\n",
    "            \n",
    "            # Read raw train data from BQ and cleanup\n",
    "            raw_train_data = (\n",
    "              pipeline\n",
    "              | '{} - Read Data from BigQuery'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "              | '{} - Extract Features'.format(step) >> beam.Map(get_features)\n",
    "            )\n",
    "            \n",
    "            # create a train dataset from the data and schema\n",
    "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "            \n",
    "            # analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn\n",
    "            transformed_train_dataset, transform_fn = (\n",
    "                raw_train_dataset \n",
    "                | '{} - Analyze & Transform'.format(step) >> impl.AnalyzeAndTransformDataset(preprocessing_fn)\n",
    "            )\n",
    "            \n",
    "            # get data and schema separately from the transformed_train_dataset\n",
    "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "            # write transformed train data to sink\n",
    "            _ = (\n",
    "                transformed_train_data \n",
    "                | '{} - Write Transformed Data as tfrecords'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    num_shards=25,\n",
    "                    coder=tft_coders.example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "            \n",
    "            \n",
    "#             #### TEST write transformed AS TEXT train data to sink\n",
    "#             _ = (\n",
    "#                 transformed_train_data \n",
    "#                 | '{} - Write Transformed Data as Text'.format(step) >> beam.io.textio.WriteToText(\n",
    "#                     file_path_prefix=Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX,\n",
    "#                     file_name_suffix=\".csv\")\n",
    "#             )\n",
    "#             ##################################################\n",
    "\n",
    "\n",
    "            ###### transform eval ##################################################################\n",
    "            \n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"\")\n",
    "                print(\"Transform eval data....\")\n",
    "                print(\"\")\n",
    "            \n",
    "            step = 'eval'\n",
    "            source_query = get_source_query(step)\n",
    "\n",
    "            # Read raw eval data from BQ and cleanup\n",
    "            raw_eval_data = (\n",
    "              pipeline\n",
    "              | '{} - Read Data from BigQuery'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "              | '{} - Extract Features'.format(step) >> beam.Map(get_features)\n",
    "            )\n",
    "            \n",
    "            # create a eval dataset from the data and schema\n",
    "            raw_eval_dataset = (raw_eval_data, raw_metadata)\n",
    "            \n",
    "            # transform eval data based on produced transform_fn (from analyzing train_data)\n",
    "            transformed_eval_dataset = (\n",
    "                (raw_eval_dataset, transform_fn) \n",
    "                | '{} - Transform'.format(step) >> impl.TransformDataset()\n",
    "            )\n",
    "            \n",
    "            # get data from the transformed_eval_dataset\n",
    "            transformed_eval_data, _ = transformed_eval_dataset\n",
    "            \n",
    "            # write transformed eval data to sink\n",
    "            _ = (\n",
    "                transformed_eval_data \n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    num_shards=10,\n",
    "                    coder=tft_coders.example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "        \n",
    "            ###### write transformation metadata #######################################################\n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"\")\n",
    "                print(\"Saving transformation artefacts ....\")\n",
    "                print(\"\")\n",
    "            \n",
    "            # write transform_fn as tf.graph\n",
    "            _ = (\n",
    "                transform_fn \n",
    "                | 'Write Transform Artefacts' >> transform_fn_io.WriteTransformFn(Params.TRANSFORM_ARTEFACTS_DIR)\n",
    "            )\n",
    "\n",
    "    if runner=='DataflowRunner':\n",
    "        pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching DirectRunner job preprocess-hackernews-data-180514-115222 ... hang on\n",
      "Sink train data files: data/news/transformed/train\n",
      "Sink data files: data/news/transformed/eval\n",
      "Temporary directory: data/news/tmp\n",
      "\n",
      "\n",
      "Transform training data....\n",
      "\n",
      "\n",
      "Transform eval data....\n",
      "\n",
      "\n",
      "Saving transformation artefacts ....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalidsalama/Technology/python-venvs/py27-venv/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:337: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported\n",
      "  pipeline.replace_all(_get_transform_overrides(pipeline.options))\n",
      "WARNING:root:Dataset ksalama-gcp-playground:temp_dataset_151e64fa07a3490bae91dd844ce4b7da does not exist so we will create it as temporary with location=None\n",
      "WARNING:root:Dataset ksalama-gcp-playground:temp_dataset_f3701d6e27e14e068968a255f43c4b8c does not exist so we will create it as temporary with location=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipline completed.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "job_name = 'preprocess-hackernews-data' + '-' + datetime.utcnow().strftime('%y%m%d-%H%M%S')\n",
    "\n",
    "options = {\n",
    "    'region': Params.REGION,\n",
    "    'staging_location': os.path.join(Params.TEMP_DIR, 'staging'),\n",
    "    'temp_location': Params.TEMP_DIR,\n",
    "    'job_name': job_name,\n",
    "    'project': Params.GCP_PROJECT_ID\n",
    "}\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "runner = 'DirectRunner' if Params.PLATFORM == 'local' else 'DirectRunner'\n",
    "\n",
    "if Params.TRANSFORM:\n",
    "    \n",
    "    if Params.PLATFORM == 'local':\n",
    "        shutil.rmtree(Params.TRANSFORMED_DATA_DIR, ignore_errors=True)\n",
    "        shutil.rmtree(Params.TRANSFORM_ARTEFACTS_DIR, ignore_errors=True)\n",
    "        shutil.rmtree(Params.TEMP_DIR, ignore_errors=True)\n",
    "    \n",
    "    print 'Launching {} job {} ... hang on'.format(runner, job_name)\n",
    "    \n",
    "    run_pipeline(runner, opts)\n",
    "    \n",
    "    print \"Pipline completed.\"\n",
    "else:\n",
    "    print \"Transformation skipped!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** transformed data:\n",
      "eval-00000-of-00010.tfrecords\n",
      "eval-00001-of-00010.tfrecords\n",
      "eval-00002-of-00010.tfrecords\n",
      "eval-00003-of-00010.tfrecords\n",
      "eval-00004-of-00010.tfrecords\n",
      "eval-00005-of-00010.tfrecords\n",
      "eval-00006-of-00010.tfrecords\n",
      "eval-00007-of-00010.tfrecords\n",
      "eval-00008-of-00010.tfrecords\n",
      "eval-00009-of-00010.tfrecords\n",
      "train-00000-of-00025.tfrecords\n",
      "train-00001-of-00025.tfrecords\n",
      "train-00002-of-00025.tfrecords\n",
      "train-00003-of-00025.tfrecords\n",
      "train-00004-of-00025.tfrecords\n",
      "train-00005-of-00025.tfrecords\n",
      "train-00006-of-00025.tfrecords\n",
      "train-00007-of-00025.tfrecords\n",
      "train-00008-of-00025.tfrecords\n",
      "train-00009-of-00025.tfrecords\n",
      "train-00010-of-00025.tfrecords\n",
      "train-00011-of-00025.tfrecords\n",
      "train-00012-of-00025.tfrecords\n",
      "train-00013-of-00025.tfrecords\n",
      "train-00014-of-00025.tfrecords\n",
      "train-00015-of-00025.tfrecords\n",
      "train-00016-of-00025.tfrecords\n",
      "train-00017-of-00025.tfrecords\n",
      "train-00018-of-00025.tfrecords\n",
      "train-00019-of-00025.tfrecords\n",
      "train-00020-of-00025.tfrecords\n",
      "train-00021-of-00025.tfrecords\n",
      "train-00022-of-00025.tfrecords\n",
      "train-00023-of-00025.tfrecords\n",
      "train-00024-of-00025.tfrecords\n",
      "\n",
      "** transform artefacts:\n",
      "transform_fn\n",
      "transformed_metadata\n",
      "\n",
      "** transform assets:\n",
      "vocab_string_to_int_uniques\n",
      "\n",
      "the\n",
      "a\n",
      "to\n",
      "for\n",
      "in\n",
      "of\n",
      "and\n",
      "s\n",
      "on\n",
      "with\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"** transformed data:\"\n",
    "ls data/news/transformed\n",
    "echo \"\"\n",
    "\n",
    "echo \"** transform artefacts:\"\n",
    "ls models/news/transform\n",
    "echo \"\"\n",
    "\n",
    "echo \"** transform assets:\"\n",
    "ls models/news/transform/transform_fn/assets\n",
    "echo \"\"\n",
    "\n",
    "head models/news/transform/transform_fn/assets/vocab_string_to_int_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
