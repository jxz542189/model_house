{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Keras) Multi-GPU Traiing for CIFAR10 dataset\n",
    "\n",
    "Helpful references:\n",
    "- https://www.tensorflow.org/guide/keras#multiple_gpus\n",
    "- https://keras.io/losses/#sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(example):\n",
    "    feature={'label': tf.FixedLenFeature((), tf.int64),\n",
    "             'image': tf.FixedLenFeature((), tf.string, default_value=\"\")}\n",
    "    parsed = tf.parse_single_example(example, feature)\n",
    "    image = tf.decode_raw(parsed['image'],tf.float64)\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image = tf.reshape(image,[32,32,3])\n",
    "    return image, parsed['label']\n",
    "\n",
    "def image_scaling(x):\n",
    "    return tf.image.per_image_standardization(x)\n",
    "\n",
    "def distort(x):\n",
    "    x = tf.image.resize_image_with_crop_or_pad(x, 40, 40)\n",
    "    x = tf.random_crop(x, [32, 32, 3])\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_input_fn(params):\n",
    "    dataset = tf.data.TFRecordDataset(\n",
    "        params['filenames'],num_parallel_reads=params['threads'])\n",
    "    dataset = dataset.map(parse_tfrecord, num_parallel_calls=params['threads'])\n",
    "    dataset = dataset.map(lambda x,y: (image_scaling(x),y),num_parallel_calls=params['threads'])\n",
    "    if params['mode']==tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.map(lambda x,y: (distort(x),y),num_parallel_calls=params['threads'])\n",
    "        dataset = dataset.shuffle(buffer_size=params['shuffle_buff'])\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(params['batch'])\n",
    "    dataset = dataset.prefetch(8*params['batch'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = tf.gfile.Glob('./data/cifar10_data_00*.tfrecords')\n",
    "eval_files = tf.gfile.Glob('./data/cifar10_data_01*.tfrecords')\n",
    "\n",
    "train_params = {'filenames': train_files,\n",
    "                'mode': tf.estimator.ModeKeys.TRAIN,\n",
    "                'threads': 16,\n",
    "                'shuffle_buff': 10000,\n",
    "                'batch': 200}\n",
    "\n",
    "eval_params  = {'filenames': eval_files,\n",
    "                'mode': tf.estimator.ModeKeys.EVAL,\n",
    "                'threads': 8,\n",
    "                'batch': 200}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "model_params  = {'drop_out': 0.2, 'dense_units': 1024, 'learning_rate': 1e-3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(128, (5, 5), padding='same', input_shape=[32, 32, 3]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128, (5,5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(model_params['dense_units']))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Dropout(model_params['drop_out']))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "opt = tf.train.GradientDescentOptimizer(model_params['learning_rate'])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator from Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'cnn_model/keras_model_'\n",
    "name = name + 'dense' + str(model_params['dense_units']) + '_'\n",
    "name = name + 'drop' + str(model_params['drop_out']) + '_'\n",
    "name = name + 'lr' + str(model_params['learning_rate']) + '_'\n",
    "name = name + time.strftime(\"%Y%m%d%H%M%S\")\n",
    "model_dir  = os.path.join('./',name)\n",
    "\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=8)\n",
    "\n",
    "config = tf.estimator.RunConfig(\n",
    "    save_checkpoints_secs = 300,\n",
    "    keep_checkpoint_max = 5,\n",
    "    session_config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True),\n",
    "    train_distribute = distribution)\n",
    "\n",
    "estimator = keras.estimator.model_to_estimator(\n",
    "  keras_model=model, config=config, model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metric to estimator\n",
    "\n",
    "def my_accuracy(labels, predictions):\n",
    "    pred_values = tf.argmax(predictions[model.output_names[0]], axis=1)\n",
    "    return {'accuracy': tf.metrics.accuracy(labels, pred_values)}\n",
    "\n",
    "estimator = tf.contrib.estimator.add_metrics(estimator, my_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on 8 GPUs\n",
    "estimator.train(input_fn=lambda: dataset_input_fn(train_params),max_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on 1 GPU\n",
    "estimator.evaluate(input_fn=lambda: dataset_input_fn(eval_params),steps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
