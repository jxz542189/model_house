{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download CIFAR-10 dataset and convert it to TFRecord format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('x_test shape : {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Test and Train data sets.  We will shuffle and redistribute later.\n",
    "X = np.concatenate([x_train, x_test]).astype(float)\n",
    "Y = np.concatenate([y_train, y_test]).astype(int)\n",
    "\n",
    "# Keras downloads labels into [n,1], reshape to [n].\n",
    "Y = Y.reshape([Y.shape[0]])\n",
    "\n",
    "# Create shuffled index toi reorder dta randomly.\n",
    "ix = list(range(X.shape[0]))\n",
    "shuffle(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "else:\n",
    "    print('directory \"data\" already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_size = 5000\n",
    "\n",
    "partition = [x for x in range(0, X.shape[0], shard_size)] + [X.shape[0]]\n",
    "data_range = zip(partition[:-1], partition[1:])\n",
    "\n",
    "data_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for creating tfrecords files\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through filenames and serialize images\n",
    "\n",
    "f_prefix = './data/cifar10_data_'\n",
    "f_digits = 3\n",
    "f_suffix = 0\n",
    "\n",
    "for _,(start,end) in enumerate(data_range):\n",
    "    f_name = f_prefix + str(f_suffix).zfill(f_digits) + '.tfrecords'\n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(f_name)\n",
    "    images = X[start:end, :, :, :]\n",
    "    labels = Y[start:end]\n",
    "    \n",
    "    for i in range(images.shape[0]):\n",
    "        image = images[i, :, :, :]\n",
    "        label = labels[i]\n",
    "        \n",
    "        e = tf.train.Example(features=tf.train.Features(\n",
    "            feature={\n",
    "                'label': _int64_feature(label),\n",
    "                'image': _bytes_feature(image.tostring())\n",
    "            }))\n",
    "        writer.write(e.SerializeToString())   \n",
    "    writer.close()\n",
    "    print('finished writing {}'.format(f_name))\n",
    "    f_suffix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
