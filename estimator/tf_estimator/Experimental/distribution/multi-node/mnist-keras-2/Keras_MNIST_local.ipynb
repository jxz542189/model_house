{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run a Keras model locally\n",
    "\n",
    "Assumes images have been preprocessed and converted to tfrecords<br/>\n",
    "Converts Keras model to Estimator using model_to_estimator<br/>\n",
    "Saves the model in GCS<br/>\n",
    "Run prediction method (locally) from the saved model<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=''\n",
    "MODEL_NAME=''\n",
    "TRAIN_STEPS=1000\n",
    "BATCH_SIZE = 50\n",
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the directories on GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "try:\n",
    "  ROOT_DIR = 'gs://{}'.format(BUCKET_NAME)\n",
    "except NameError:\n",
    "  ROOT_DIR = './tutorial'\n",
    "  \n",
    "DATA_DIR = '{}/data'.format(ROOT_DIR)\n",
    "MODEL_DIR = '{}/{}'.format(ROOT_DIR, MODEL_NAME)\n",
    "EXPORT_DIR = '{}/{}'.format(ROOT_DIR, MODEL_NAME)\n",
    "CHECKPOINT_PATH = '{}/checkpoint'.format(MODEL_DIR)\n",
    "\n",
    "os.environ['BUCKET_NAME']=BUCKET_NAME\n",
    "os.environ['MODEL_NAME']=MODEL_NAME\n",
    "  \n",
    "# Remove CHECKPOINT_DIR if needed\n",
    "if tf.gfile.IsDirectory(MODEL_DIR):\n",
    "  tf.logging.info('delete {}'.format(MODEL_DIR))\n",
    "  tf.gfile.DeleteRecursively(MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input function using tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_fn(file_pattern, mode, batch_size=BATCH_SIZE, count=N_EPOCHS):\n",
    "  \n",
    "  def parse_record(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.string),            \n",
    "        })\n",
    "    # Normalize from [0, 255] to [0.0, 1.0]\n",
    "    image = tf.decode_raw(features['image'], tf.uint8)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [28*28]) / 255.0\n",
    "    label = tf.decode_raw(features['label'], tf.uint8)\n",
    "    label = tf.reshape(label, [])\n",
    "    label = tf.one_hot(label, 10, dtype=tf.int32)\n",
    "    return image, label\n",
    "\n",
    "  def input_fn():\n",
    "    files = tf.data.Dataset.list_files(file_pattern)\n",
    "    dataset = tf.data.TFRecordDataset(files)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      dataset = dataset.cache()\n",
    "      dataset = dataset.shuffle(10000)\n",
    "      dataset = dataset.repeat(count=count)\n",
    "      \n",
    "    dataset = dataset.map(parse_record)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "\n",
    "    return features, labels\n",
    "  \n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and return the estimator and input_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(MODEL_DIR):\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Dense(300, activation='relu', input_shape=[28*28]))\n",
    "  model.add(tf.keras.layers.Dense(100, activation='relu'))  \n",
    "  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.SGD(lr=0.005),\n",
    "                metrics=['accuracy'])  \n",
    "  estimator = tf.keras.estimator.model_to_estimator(\n",
    "    model, model_dir=MODEL_DIR)\n",
    "\n",
    "  input_signature = model.input.name.split(':')[0]\n",
    "  \n",
    "  return estimator, input_signature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_serving_input_fn(input_signature):\n",
    "  def preprocess(x):\n",
    "    return tf.reshape(x, [-1, 28*28]) / 255.0\n",
    "\n",
    "  def serving_input_fn():\n",
    "    receiver_tensor = {'X': tf.placeholder(tf.float32, shape=[None, 28, 28])}\n",
    "    features = {input_signature: tf.map_fn(preprocess, receiver_tensor['X'])}\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)\n",
    "  return serving_input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input functions for train, eval, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = generate_input_fn(\n",
    "    file_pattern='{}/train.tfrecord'.format(DATA_DIR),\n",
    "    mode=tf.estimator.ModeKeys.TRAIN,\n",
    "    batch_size=BATCH_SIZE, count=N_EPOCHS)\n",
    "\n",
    "eval_input_fn = generate_input_fn(\n",
    "    file_pattern='{}/eval.tfrecord'.format(DATA_DIR),\n",
    "    mode=tf.estimator.ModeKeys.EVAL, count=1)\n",
    "\n",
    "test_input_fn = generate_input_fn(\n",
    "    file_pattern='{}/test.tfrecord'.format(DATA_DIR),\n",
    "    mode=tf.estimator.ModeKeys.PREDICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training and evaluation using the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the estimator and input signature from the keras model conversion to tf.estimator\n",
    "estimator, input_signature = get_estimator(MODEL_DIR)\n",
    "\n",
    "# exporter is invoked during evaluation\n",
    "exporter = tf.estimator.LatestExporter(\n",
    "    name='export',\n",
    "    serving_input_receiver_fn=get_serving_input_fn(input_signature))\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=eval_input_fn,\n",
    "    steps=None,\n",
    "    start_delay_secs=60,\n",
    "    throttle_secs=60,\n",
    "    exporters=exporter)\n",
    "\n",
    "# finally, train and evaluate\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the model location and signature of the serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET_NAME}/${MODEL_NAME}/export/ | tail -1)\n",
    "echo \"export model location:\" ${MODEL_LOCATION}\n",
    "\n",
    "saved_model_cli show --dir ${MODEL_LOCATION} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the values for the export directory and output key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the export model location\n",
    "export_dir = \"\"\n",
    "\n",
    "#get the tensor output key, for example 'dense_3' from the above output\n",
    "outputs_key=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with test data locally from the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "N_EXAMPLES = 100\n",
    "\n",
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "X_train = train[0][:-5000]\n",
    "y_train = train[1][:-5000]\n",
    "X_eval = train[0][-5000:]\n",
    "y_eval = train[1][-5000:]\n",
    "X_test = test[0]\n",
    "y_test = test[1]\n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir=export_dir, signature_def_key='serving_default')\n",
    "\n",
    "_X = X_test[:N_EXAMPLES]\n",
    "_y = y_test[:N_EXAMPLES]\n",
    "\n",
    "output = predictor_fn({'X': _X})\n",
    "class_ids = np.argmax(output[outputs_key], axis=2).reshape(-1)\n",
    "\n",
    "accuracy = np.sum(_y == class_ids)/float(N_EXAMPLES)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
