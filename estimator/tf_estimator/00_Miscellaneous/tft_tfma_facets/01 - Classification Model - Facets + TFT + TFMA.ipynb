{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Classification Model using TF 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show the following:\n",
    "* How to use **facets** to visualise your dataset\n",
    "* Preprocess and transform raw data using **tf.transform** to .tfrecords\n",
    "* Create **feature columns** based on the **transformed_metadata** produced by tf.tramsform\n",
    "* Build a classification model using **DNNLinearCombinedClassifier**\n",
    "* Use the **transform_fn** produced by tf.tramsform in json **serving_fn**\n",
    "* Visualise you model evaluation using **TensorFlow Model Analysis**\n",
    "* Use the SavedModel for predictions\n",
    "\n",
    "Please run **\"00 - install tools and libraries.ipynb\"** first to perpare your environement, then restart your kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Bank Marketing - UCI Dataset Repository\n",
    "https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \n",
    "* 45211 instances\n",
    "* 20 (mixed) features\n",
    "* 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir data\n",
    "mkdir models\n",
    "mkdir tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Save Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "DOWNLOAD_DATA = False\n",
    "\n",
    "if DOWNLOAD_DATA:\n",
    "\n",
    "    testfile = urllib.URLopener()\n",
    "    testfile.retrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\", \n",
    "                      \"data/bank-data.zip\")\n",
    "\n",
    "    print \"Zip data was downladed.\"\n",
    "\n",
    "    zip_ref = zipfile.ZipFile(\"data/bank-data.zip\", 'r')\n",
    "    zip_ref.extractall(\"data\")\n",
    "    zip_ref.close()\n",
    "\n",
    "    print \"Data was unzipped.\"\n",
    "\n",
    "    shutil.move('data/bank-additional/bank-additional-full.csv', 'data/bank-train-01.csv')\n",
    "    shutil.move('data/bank-additional/bank-additional.csv', 'data/bank-eval-01.csv')\n",
    "\n",
    "    shutil.rmtree('data/bank-additional/', ignore_errors=True)\n",
    "\n",
    "    os.remove('data/bank-data.zip')\n",
    "\n",
    "print \"Raw data file are ready.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "head data/bank-train-01.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tutorial-wide Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Params:\n",
    "    pass\n",
    "\n",
    "# Set to run on GCP\n",
    "Params.GCP_PROJECT_ID = ''\n",
    "\n",
    "# change to GCS location to run on GCP\n",
    "Params.DATA_DIR = 'data'\n",
    "Params.TRANSFORMED_DATA_DIR = 'data/transformed'\n",
    "\n",
    "Params.RAW_DATA_DELIMITER = ';'\n",
    "Params.RAW_TRAIN_DATA_FILE = os.path.join(Params.DATA_DIR, 'bank-train-01.csv')\n",
    "Params.RAW_EVAL_DATA_FILE = os.path.join(Params.DATA_DIR, 'bank-eval-01.csv')\n",
    "\n",
    "Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX = os.path.join(Params.TRANSFORMED_DATA_DIR, 'bank-train')\n",
    "Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX = os.path.join(Params.TRANSFORMED_DATA_DIR, 'bank-eval')\n",
    "\n",
    "# change to GCS location to run on GCP\n",
    "Params.TEMP_DIR = 'tmp'\n",
    "\n",
    "# change to GCS location to run on GCP\n",
    "Params.MODELS_DIR = 'models'\n",
    "\n",
    "Params.TRANSFORM_ARTEFACTS_DIR = os.path.join(Params.MODELS_DIR,'transform')\n",
    "\n",
    "Params.TRANSFORM = True\n",
    "\n",
    "Params.TRAIN = True\n",
    "\n",
    "Params.EXTEND_FEATURE_COLUMNS = True\n",
    "\n",
    "Params.RESUME_TRAINING = True\n",
    "\n",
    "Params.DISPLAY_FACETS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "headers = [\n",
    "    'age', 'job', 'marital_status', 'education', \n",
    "    'has_credit_default', 'has_housing_loan',\n",
    "    'has_personal_loan', 'contact_type', 'last_contact_month', \n",
    "    'last_contact_day_of_week', 'last_contact_duration', \n",
    "    'campaign_contact_count','days_since_last_contact', \n",
    "    'previous_campaign_contact_count', 'previous_campaign_outcome', \n",
    "    'employment_variation_rate', 'consumer_price_index','consumer_confidence_index', \n",
    "    'euribor3m', 'number_of_employees', \n",
    "    'has_subscribed'\n",
    "]\n",
    "data_train = pd.read_csv(Params.RAW_TRAIN_DATA_FILE, sep=\";\")\n",
    "data_train.columns = headers\n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Dataset using Facets - Big Picture\n",
    "visit: https://research.google.com/bigpicture/\n",
    "\n",
    "* Use Stacked with categorical features to test the distribution  If used with numerical features will bucketise them.\n",
    "* Use Scatter with numerical values to test correlations.\n",
    "* Use Facets to slice and dice (vertically and horizontally).\n",
    "* Use colour with the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "if Params.DISPLAY_FACETS:\n",
    "    \n",
    "    jsonstr = data_train.to_json(orient='records')\n",
    "\n",
    "    HTML_TEMPLATE = \"\"\"<link rel=\"import\" href=\"/nbextensions/facets-dist/facets-jupyter.html\">\n",
    "            <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n",
    "            <script>\n",
    "              var data = {jsonstr};\n",
    "              document.querySelector(\"#elem\").data = data;\n",
    "            </script>\"\"\"\n",
    "    html = HTML_TEMPLATE.format(jsonstr=jsonstr)\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dist_train = data_train.has_subscribed.value_counts()\n",
    "base_accuracy = float(max(class_dist_train))/sum(class_dist_train)\n",
    "print \"baseline accuracy - train: {}%\".format(round(base_accuracy*100,2))\n",
    "\n",
    "data_eval = pd.read_csv(Params.RAW_EVAL_DATA_FILE, sep=\";\")\n",
    "data_eval.columns = headers\n",
    "\n",
    "class_dist_eval = data_eval.has_subscribed.value_counts()\n",
    "base_accuracy = float(max(class_dist_eval))/sum(class_dist_eval)\n",
    "print \"baseline accuracy - eval: {}%\".format(round(base_accuracy*100,2))\n",
    "\n",
    "del data_train\n",
    "del data_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TensorFlow & related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform.coders as tft_coders\n",
    "\n",
    "from tensorflow_transform.beam import impl\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow.contrib.learn.python.learn.utils import input_fn_utils\n",
    "\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.saved import saved_transform_io\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Raw Data Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Declare raw features types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_FEATURE_NAMES = [\n",
    "    'age', 'job', 'marital_status', 'education', \n",
    "    'has_credit_default', 'has_housing_loan',\n",
    "    'has_personal_loan', 'contact_type', 'last_contact_month', \n",
    "    'last_contact_day_of_week', 'last_contact_duration', \n",
    "    'campaign_contact_count','days_since_last_contact', \n",
    "    'previous_campaign_contact_count', 'previous_campaign_outcome', \n",
    "    'employment_variation_rate', 'consumer_price_index','consumer_confidence_index', \n",
    "    'euribor3m', 'number_of_employees', \n",
    "    'has_subscribed'\n",
    "]\n",
    "\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['age', 'last_contact_duration', 'campaign_contact_count',\n",
    "    'days_since_last_contact', 'previous_campaign_contact_count',\n",
    "    'employment_variation_rate', 'consumer_price_index',\n",
    "    'consumer_confidence_index', 'euribor3m', 'number_of_employees'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES = ['contact_type','education','has_credit_default',\n",
    "    'has_housing_loan', 'has_personal_loan', 'job', 'last_contact_day_of_week',\n",
    "    'last_contact_month', 'marital_status','previous_campaign_outcome'\n",
    "]\n",
    "\n",
    "TARGET_FEATURE_NAME = 'has_subscribed'\n",
    "\n",
    "TARGET_LABELS = ['yes', 'no']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create a tf.transform metadata object (including Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_metadata():\n",
    "    \n",
    "    column_schemas = {}\n",
    "    \n",
    "    # ColumnSchema for numeric features\n",
    "    column_schemas.update({\n",
    "      key: dataset_schema.ColumnSchema(\n",
    "          tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "      for key in NUMERIC_FEATURE_NAMES\n",
    "    })\n",
    "    \n",
    "    # ColumnSchema for categorical features\n",
    "    column_schemas.update({\n",
    "      key: dataset_schema.ColumnSchema(\n",
    "          tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "      for key in CATEGORICAL_FEATURE_NAMES\n",
    "    })\n",
    "    \n",
    "    # ColumnSchema for target feature\n",
    "    column_schemas[TARGET_FEATURE_NAME] = dataset_schema.ColumnSchema(\n",
    "        tf.string, [], \n",
    "        dataset_schema.FixedColumnRepresentation()\n",
    "    )\n",
    "    \n",
    "    # Dataset Metadata\n",
    "    raw_metadata = dataset_metadata.DatasetMetadata(\n",
    "        dataset_schema.Schema(column_schemas)\n",
    "    )\n",
    "    \n",
    "    return raw_metadata\n",
    "\n",
    "#create_raw_metadata().schema.as_feature_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BUCKETS = 4\n",
    "\n",
    "def preprocess(input_features):\n",
    "    \n",
    "    output_features = {}\n",
    "    \n",
    "    output_features[TARGET_FEATURE_NAME] = input_features[TARGET_FEATURE_NAME]\n",
    "    \n",
    "    for feature_name in NUMERIC_FEATURE_NAMES:\n",
    "        \n",
    "        output_features[feature_name+\"_scaled\"] = tft.scale_to_0_1(input_features[feature_name])\n",
    "        \n",
    "        quantiles = tft.quantiles(input_features[feature_name], num_buckets=NUM_BUCKETS, epsilon=0.01)\n",
    "        output_features[feature_name+\"_bucketized\"] = tft.apply_buckets(input_features[feature_name], \n",
    "                                                                        bucket_boundaries=quantiles)\n",
    "\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        \n",
    "        tft.uniques(input_features[feature_name], vocab_filename=feature_name)\n",
    "        output_features[feature_name] = input_features[feature_name]\n",
    "        \n",
    "#         output_features[feature_name+\"_integerized\"] = tft.string_to_int(input_features[feature_name],\n",
    "#                                                           vocab_filename=feature_name)\n",
    "    \n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transformation Beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def run_transformation_pipeline(runner, options):\n",
    "    \n",
    "    options = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    \n",
    "    print(\"Source raw train data files: {}\".format(Params.RAW_TRAIN_DATA_FILE))\n",
    "    print(\"Source raw train data files: {}\".format(Params.RAW_EVAL_DATA_FILE))\n",
    "\n",
    "    print(\"Sink transformed train data files: {}\".format(Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX))\n",
    "    print(\"Sink transformed data files: {}\".format(Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX))\n",
    "    print(\"Sink transform artefacts directory: {}\".format(Params.TRANSFORM_ARTEFACTS_DIR))\n",
    "   \n",
    "    print(\"Temporary directory: {}\".format(Params.TEMP_DIR))\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "    with beam.Pipeline(runner, options=options) as pipeline:\n",
    "        with impl.Context(Params.TEMP_DIR):\n",
    "            \n",
    "            raw_metadata = create_raw_metadata()\n",
    "            converter = tft_coders.csv_coder.CsvCoder(column_names=RAW_FEATURE_NAMES,\n",
    "                                                      delimiter=Params.RAW_DATA_DELIMITER, \n",
    "                                                      schema=raw_metadata.schema)\n",
    "            \n",
    "            ###### analyze & transform train #########################################################\n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"Transform training data....\")\n",
    "            \n",
    "            step = 'train'\n",
    "\n",
    "            # Read raw train data from csv files\n",
    "            raw_train_data = (\n",
    "              pipeline\n",
    "              | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(Params.RAW_TRAIN_DATA_FILE)\n",
    "              | '{} - Remove Empty Rows'.format(step) >> beam.Filter(lambda line: line)\n",
    "              | '{} - Decode CSV Data'.format(step) >> beam.Map(converter.decode)\n",
    "            \n",
    "            )\n",
    "            \n",
    "            # create a train dataset from the data and schema\n",
    "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "            \n",
    "            # analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn\n",
    "            transformed_train_dataset, transform_fn = (\n",
    "                raw_train_dataset \n",
    "                | '{} - Analyze & Transform'.format(step) >> impl.AnalyzeAndTransformDataset(preprocess)\n",
    "            )\n",
    "            \n",
    "            # get data and schema separately from the transformed_train_dataset\n",
    "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "            # write transformed train data to sink\n",
    "            _ = (\n",
    "                transformed_train_data \n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    coder=tft_coders.example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "            \n",
    "            ###### transform eval ##################################################################\n",
    "            \n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"Transform eval data....\")\n",
    "            \n",
    "            step = 'eval'\n",
    "\n",
    "            raw_eval_data = (\n",
    "              pipeline\n",
    "              | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(Params.RAW_EVAL_DATA_FILE)\n",
    "              | '{} - Remove Empty Lines'.format(step) >> beam.Filter(lambda line: line)\n",
    "              | '{} - Decode CSV Data'.format(step) >> beam.Map(converter.decode)\n",
    "            \n",
    "            )\n",
    "            \n",
    "            # create a eval dataset from the data and schema\n",
    "            raw_eval_dataset = (raw_eval_data, raw_metadata)\n",
    "            \n",
    "            # transform eval data based on produced transform_fn (from analyzing train_data)\n",
    "            transformed_eval_dataset = (\n",
    "                (raw_eval_dataset, transform_fn) \n",
    "                | '{} - Transform'.format(step) >> impl.TransformDataset()\n",
    "            )\n",
    "            \n",
    "            # get data from the transformed_eval_dataset\n",
    "            transformed_eval_data, _ = transformed_eval_dataset\n",
    "            \n",
    "            # write transformed eval data to sink\n",
    "            _ = (\n",
    "                transformed_eval_data \n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    coder=tft_coders.example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "            \n",
    "        \n",
    "            ###### write transformation metadata #######################################################\n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"Saving transformation artefacts ....\")\n",
    "            \n",
    "            # write transform_fn as tf.graph\n",
    "            _ = (\n",
    "                transform_fn \n",
    "                | 'Write Transform Artefacts' >> transform_fn_io.WriteTransformFn(Params.TRANSFORM_ARTEFACTS_DIR)\n",
    "            )\n",
    "\n",
    "    if runner=='DataflowRunner':\n",
    "        pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile requirements.txt\n",
    "# tensorflow-transform==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "if Params.TRANSFORM:\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    runner = 'DirectRunner' # DirectRunner | DataflowRunner\n",
    "\n",
    "    job_name = 'preprocess-data-tft-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "    print 'Launching {} job {} ... hang on'.format(runner, job_name)\n",
    "    print(\"\")\n",
    "\n",
    "    options = {\n",
    "        'region': 'europe-west1',\n",
    "        'staging_location': os.path.join(Params.DATA_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(Params.DATA_DIR, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': Params.GCP_PROJECT_ID,\n",
    "        'worker_machine_type': 'n1-standard-1',\n",
    "        'max_num_workers': 20,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "        'requirements_file': 'requirements.txt',\n",
    "    }\n",
    "\n",
    "    if runner == 'DirectRunner':\n",
    "\n",
    "        shutil.rmtree(Params.TRANSFORM_ARTEFACTS_DIR, ignore_errors=True)\n",
    "        shutil.rmtree(Params.TRANSFORMED_DATA_DIR, ignore_errors=True)\n",
    "        shutil.rmtree(Params.TEMP_DIR, ignore_errors=True)\n",
    "\n",
    "\n",
    "        run_transformation_pipeline(runner, options)\n",
    "        print(\"Transformation is done!\")\n",
    "else:\n",
    "    print(\"Transformation was skipped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"** transformed data:\"\n",
    "ls data/transformed\n",
    "echo \"\"\n",
    "\n",
    "echo \"** transform artefacts:\"\n",
    "ls models/transform\n",
    "echo \"\"\n",
    "\n",
    "echo \"** transform assets:\"\n",
    "ls models/transform/transform_fn/assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_file_by_name(transform_artefacts_dir, key):\n",
    "    return os.path.join(\n",
    "        transform_artefacts_dir,\n",
    "        transform_fn_io.TRANSFORM_FN_DIR,\n",
    "        'assets',\n",
    "        key.replace('_integerized',''))\n",
    "\n",
    "\n",
    "def get_vocabulary_size_by_name(transform_artefacts_dir, key):\n",
    "    vocabulary = get_vocabulary_file_by_name(transform_artefacts_dir, key)\n",
    "    with tf.gfile.Open(vocabulary, 'r') as f:\n",
    "        return sum(1 for _ in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "print \"tensorflow version: {}\".format(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Data Input Funtion for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_metadata = metadata_io.read_metadata(\n",
    "    os.path.join(Params.TRANSFORM_ARTEFACTS_DIR,\"transformed_metadata\"))\n",
    "\n",
    "transformed_feature_spec = transformed_metadata.schema.as_feature_spec()\n",
    "\n",
    "#print(transformed_feature_spec)\n",
    "\n",
    "TRANSFORMED_NUMERIC_FEATURE_NAMES = [ \n",
    "    feature_name \n",
    "    for  feature_name in transformed_feature_spec.keys() \n",
    "    if feature_name.endswith('_scaled')\n",
    "]\n",
    "                                    \n",
    "\n",
    "TRANSFORMED_BUCKETIZED_FEATURE_NAMES = [ \n",
    "    feature_name \n",
    "    for  feature_name in transformed_feature_spec.keys() \n",
    "    if feature_name.endswith('_bucketized')\n",
    "]\n",
    "\n",
    "\n",
    "TRANSFORMED_CATEGORICAL_FEATURE_NAMES = CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "TRANSFORMED_INTEGERIZED_CATEGORICAL_FEATURE_NAMES = [ \n",
    "    feature_name \n",
    "    for  feature_name in transformed_feature_spec.keys() \n",
    "    if feature_name.endswith('_integerized')\n",
    "]\n",
    "\n",
    "print TRANSFORMED_NUMERIC_FEATURE_NAMES\n",
    "print \"\"\n",
    "print TRANSFORMED_BUCKETIZED_FEATURE_NAMES\n",
    "print \"\"\n",
    "print TRANSFORMED_CATEGORICAL_FEATURE_NAMES\n",
    "print \"\"\n",
    "print TRANSFORMED_INTEGERIZED_CATEGORICAL_FEATURE_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_example(tf_example):\n",
    "    \n",
    "    parsed_features = tf.parse_example(serialized=tf_example, features=transformed_feature_spec)\n",
    "    target = parsed_features.pop(TARGET_FEATURE_NAME)\n",
    "    \n",
    "    return parsed_features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be applied in traing and serving\n",
    "# ideally, you put this logic in preprocess_tft, to avoid transforming the records during training several times\n",
    "\n",
    "def process_features(features):\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfrecords_input_fn(files_name_pattern, \n",
    "                                mode=tf.estimator.ModeKeys.EVAL,  \n",
    "                                num_epochs=1, \n",
    "                                batch_size=500):\n",
    "    \n",
    "    def _input_fn():\n",
    "    \n",
    "        shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "\n",
    "        file_names = data.Dataset.list_files(files_name_pattern)\n",
    "\n",
    "        dataset = data.TFRecordDataset(filenames=file_names)\n",
    "        \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example))\n",
    "        dataset = dataset.map(lambda features, target: (process_features(features), target))\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "        features, target = iterator.get_next()\n",
    "        return features, target\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.feature_column import feature_column\n",
    "\n",
    "def extend_feature_columns(feature_columns, hparams):\n",
    "\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_columns(hparams):\n",
    "    \n",
    "    feature_columns = {}\n",
    "\n",
    "    numeric_columns = {\n",
    "        feature_name: tf.feature_column.numeric_column(feature_name)\n",
    "        for feature_name in TRANSFORMED_NUMERIC_FEATURE_NAMES\n",
    "    }\n",
    "    \n",
    "    bucketized_columns = {\n",
    "        feature_name: tf.feature_column.categorical_column_with_identity(feature_name, num_buckets=NUM_BUCKETS+2)\n",
    "        for feature_name in TRANSFORMED_BUCKETIZED_FEATURE_NAMES\n",
    "    }\n",
    "    \n",
    "    categorical_columns = {\n",
    "        feature_name: tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "            key=feature_name, \n",
    "            vocabulary_file=get_vocabulary_file_by_name(Params.TRANSFORM_ARTEFACTS_DIR, feature_name))\n",
    "        for feature_name in TRANSFORMED_CATEGORICAL_FEATURE_NAMES}\n",
    "    \n",
    "    integerized_columns = {\n",
    "        feature_name: tf.feature_column.categorical_column_with_identity(\n",
    "            key=feature_name, \n",
    "            num_buckets=get_vocabulary_size_by_name(Params.TRANSFORM_ARTEFACTS_DIR, feature_name))\n",
    "        for feature_name in TRANSFORMED_INTEGERIZED_CATEGORICAL_FEATURE_NAMES}\n",
    "    \n",
    "    if numeric_columns is not None:\n",
    "        feature_columns.update(numeric_columns)\n",
    "        \n",
    "    if bucketized_columns is not None:\n",
    "        feature_columns.update(bucketized_columns)\n",
    "        \n",
    "    if integerized_columns is not None:\n",
    "        feature_columns.update(integerized_columns)\n",
    "        \n",
    "    if categorical_columns is not None:\n",
    "        feature_columns.update(categorical_columns)\n",
    "\n",
    "    if Params.EXTEND_FEATURE_COLUMNS:\n",
    "        feature_columns = extend_feature_columns(feature_columns, hparams)\n",
    "        \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.feature_column import feature_column\n",
    "\n",
    "def get_wide_deep_columns(hparams):\n",
    "    \n",
    "    feature_columns = list(create_feature_columns(hparams).values())\n",
    "    \n",
    "    dense_columns = list(\n",
    "        filter(lambda column: \n",
    "                 isinstance(column, feature_column._NumericColumn) \n",
    "               | isinstance(column, feature_column._EmbeddingColumn)\n",
    "               ,feature_columns\n",
    "        )\n",
    "    )\n",
    "\n",
    "    categorical_columns = list(\n",
    "        filter(lambda column: \n",
    "                 isinstance(column, feature_column._VocabularyListCategoricalColumn) \n",
    "               | isinstance(column, feature_column._VocabularyFileCategoricalColumn) \n",
    "               | isinstance(column, feature_column._IdentityCategoricalColumn) \n",
    "               | isinstance(column, feature_column._BucketizedColumn)\n",
    "               ,feature_columns)\n",
    "    )\n",
    "    \n",
    "    sparse_columns = list(\n",
    "        filter(lambda column: \n",
    "                 isinstance(column,feature_column._HashedCategoricalColumn) \n",
    "               | isinstance(column, feature_column._CrossedColumn)\n",
    "               , feature_columns)\n",
    "    )\n",
    "\n",
    "    indicator_columns = []\n",
    "    \n",
    "    if hparams.use_indicators: \n",
    "        indicator_columns = [\n",
    "            tf.feature_column.indicator_column(column)\n",
    "            for column in categorical_columns\n",
    "        ]\n",
    "    \n",
    "    deep_feature_columns = dense_columns + indicator_columns\n",
    "    wide_feature_columns = (categorical_columns + sparse_columns) if hparams.use_wide_columns else []\n",
    "    \n",
    "    return wide_feature_columns, deep_feature_columns\n",
    "\n",
    "# get_wide_deep_columns(tf.contrib.training.HParams(\n",
    "#     use_indicators = False,\n",
    "#     use_wide_columns = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_label_column(label_string_tensor):\n",
    "    table = tf.contrib.lookup.index_table_from_tensor(\n",
    "        tf.constant(TARGET_LABELS)\n",
    "    )\n",
    "    return table.lookup(label_string_tensor)\n",
    "\n",
    "\n",
    "def metric_fn(labels, predictions):\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    indices = parse_label_column(labels)\n",
    "    pred_class = predictions['class_ids']\n",
    "    metrics['mirco_accuracy'] = tf.metrics.mean_per_class_accuracy(\n",
    "        labels=indices,\n",
    "        predictions=pred_class,\n",
    "        num_classes=len(TARGET_LABELS)\n",
    "    )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn_estimator(run_config, hparams):\n",
    "    \n",
    "    print \"creating a dnn linear combined estimator...\"\n",
    "    print \"\"\n",
    "    \n",
    "    wide_feature_columns, deep_feature_columns = get_wide_deep_columns(hparams)\n",
    "    \n",
    "#     print \"wide columns: {}\".format(wide_feature_columns)\n",
    "#     print \"\"\n",
    "#     print \"deep columns: {}\".format(deep_feature_columns)\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "        \n",
    "        n_classes= len(TARGET_LABELS),\n",
    "        label_vocabulary=TARGET_LABELS,\n",
    "        \n",
    "        dnn_feature_columns = deep_feature_columns,\n",
    "        linear_feature_columns = wide_feature_columns,\n",
    "        \n",
    "        dnn_hidden_units= hparams.hidden_units,\n",
    "        \n",
    "        dnn_optimizer= tf.train.AdamOptimizer(learning_rate=hparams.learning_rate),\n",
    "        \n",
    "        dnn_activation_fn= tf.nn.elu,\n",
    "        dnn_dropout= hparams.dropout_prob,\n",
    "        \n",
    "        config= run_config\n",
    "    )\n",
    "    \n",
    "    \n",
    "    estimator = tf.contrib.estimator.add_metrics(\n",
    "        estimator=estimator,\n",
    "        metric_fn=metric_fn\n",
    "    )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train, Evaluate, and Export Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Set HParams and RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 41188\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 500\n",
    "TOTAL_STEPS = (TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS\n",
    "EVAL_EVERY_SEC = 30\n",
    "\n",
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    \n",
    "    embedding_size = 3,\n",
    "    \n",
    "    use_indicators = False,\n",
    "    use_wide_columns = True,\n",
    "    learning_rate = 0.01,\n",
    "    \n",
    "    hidden_units=[16, 12, 8],\n",
    "    dropout_prob = 0.0,\n",
    "    \n",
    "    max_steps = TOTAL_STEPS,\n",
    "\n",
    ")\n",
    "\n",
    "MODEL_NAME = 'dnn_estimator' # 'tree_estimator' | 'dnn_estimator'\n",
    "model_dir = os.path.join(Params.MODELS_DIR, MODEL_NAME)\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    log_step_count_steps=1000,\n",
    "    save_checkpoints_secs=EVAL_EVERY_SEC,\n",
    "    keep_checkpoint_max=3,\n",
    "    model_dir=model_dir\n",
    ")\n",
    "\n",
    "\n",
    "print(hparams)\n",
    "print(\"\")\n",
    "print(\"Model Directory:\", run_config.model_dir)\n",
    "print(\"Dataset Size:\", TRAIN_SIZE)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Steps per Epoch:\",TRAIN_SIZE/BATCH_SIZE)\n",
    "print(\"Total Steps:\", TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Define serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_serving_fn():\n",
    "    \n",
    "    # get the feature_spec of raw data\n",
    "    raw_metadata = create_raw_metadata()\n",
    "    raw_placeholder_spec = raw_metadata.schema.as_batched_placeholders()\n",
    "    raw_placeholder_spec.pop(TARGET_FEATURE_NAME)\n",
    "    \n",
    "    def _serving_fn():\n",
    "\n",
    "        raw_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(raw_placeholder_spec)\n",
    "        raw_features, recevier_tensors, _ = raw_input_fn()\n",
    "\n",
    "        # apply tranform_fn on raw features\n",
    "        _, transformed_features = (\n",
    "            saved_transform_io.partially_apply_saved_transform(\n",
    "                os.path.join(Params.TRANSFORM_ARTEFACTS_DIR, transform_fn_io.TRANSFORM_FN_DIR),\n",
    "            raw_features)\n",
    "        )\n",
    "        \n",
    "        # apply the process_features function to transformed features\n",
    "        transformed_features = process_features(transformed_features)\n",
    "        \n",
    "        return tf.estimator.export.ServingInputReceiver(\n",
    "            transformed_features, raw_features)\n",
    "    \n",
    "    return _serving_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Create TrainSpec and EvalSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn = generate_tfrecords_input_fn(\n",
    "        Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX+\"*\",\n",
    "        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "        num_epochs=hparams.num_epochs,\n",
    "        batch_size=hparams.batch_size\n",
    "    ),\n",
    "    max_steps=hparams.max_steps,\n",
    "    hooks=None\n",
    ")\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = generate_tfrecords_input_fn(\n",
    "        Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX+\"*\",\n",
    "        mode=tf.estimator.ModeKeys.EVAL,\n",
    "        num_epochs=1,\n",
    "        batch_size=hparams.batch_size\n",
    "    ),\n",
    "    exporters=[tf.estimator.LatestExporter(\n",
    "        name=\"estimate\", # the name of the folder in which the model will be exported to under export\n",
    "        serving_input_receiver_fn=generate_json_serving_fn(),\n",
    "        exports_to_keep=1,\n",
    "        as_text=False)],\n",
    "    steps=None,\n",
    "    throttle_secs=EVAL_EVERY_SEC\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Params.TRAIN:\n",
    "    if not Params.RESUME_TRAINING:\n",
    "        print(\"Removing previous training artefacts...\")\n",
    "        shutil.rmtree(model_dir, ignore_errors=True)\n",
    "    else:\n",
    "        print(\"Resuming training...\") \n",
    "\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "\n",
    "    estimator = create_dnn_estimator(run_config, hparams)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec, \n",
    "        eval_spec=eval_spec\n",
    "    )\n",
    "\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "else:\n",
    "    print \"Training was skipped!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls models/dnn_estimator/export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 41188\n",
    "VALID_SIZE = 4119\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "estimator = create_dnn_estimator(run_config, hparams)\n",
    "\n",
    "train_metrics = estimator.evaluate(\n",
    "    input_fn = generate_tfrecords_input_fn(\n",
    "        files_name_pattern= Params.TRANSFORMED_TRAIN_DATA_FILE_PREFIX+\"*\", \n",
    "        mode= tf.estimator.ModeKeys.EVAL,\n",
    "        batch_size= TRAIN_SIZE), \n",
    "    steps=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Train Measures: {}\".format(train_metrics))\n",
    "print(\"############################################################################################\")\n",
    "\n",
    "eval_metrics = estimator.evaluate(\n",
    "    input_fn=generate_tfrecords_input_fn(\n",
    "        files_name_pattern= Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX+\"*\", \n",
    "        mode= tf.estimator.ModeKeys.EVAL,\n",
    "        batch_size= TRAIN_SIZE), \n",
    "    steps=1\n",
    ")\n",
    "print(\"\")\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Valid Measures: {}\".format(eval_metrics))\n",
    "print(\"############################################################################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation Analysis Using TFMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Define Evaluate input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval_receiver_fn(transform_artefacts_dir):\n",
    "    \n",
    "    transformed_metadata = metadata_io.read_metadata(transform_artefacts_dir+\"/transformed_metadata\")\n",
    "    transformed_feature_spec = transformed_metadata.schema.as_feature_spec()\n",
    "    \n",
    "    def _eval_receiver_fn():\n",
    "        \n",
    "        serialized_tf_example = tf.placeholder(\n",
    "            dtype=tf.string, shape=[None], name='input_example_placeholder')\n",
    "\n",
    "        receiver_tensors = {'examples': serialized_tf_example}\n",
    "        transformed_features = tf.parse_example(serialized_tf_example, transformed_feature_spec)\n",
    "\n",
    "        return tfma.export.EvalInputReceiver(\n",
    "            features=transformed_features,\n",
    "            receiver_tensors=receiver_tensors,\n",
    "            labels=transformed_features[TARGET_FEATURE_NAME])\n",
    "\n",
    "    return _eval_receiver_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Export Evaluation Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model_dir = model_dir +\"/export/evaluate\"\n",
    "\n",
    "shutil.rmtree(eval_model_dir, ignore_errors=True)\n",
    "\n",
    "tfma.export.export_eval_savedmodel(\n",
    "        estimator=estimator,\n",
    "        export_dir_base=eval_model_dir,\n",
    "        eval_input_receiver_fn=generate_eval_receiver_fn(Params.TRANSFORM_ARTEFACTS_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls models/dnn_estimator/export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Define Data SliceSpecs for Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_spec = [tfma.SingleSliceSpec()]\n",
    "for feature_name in TRANSFORMED_NUMERIC_FEATURE_NAMES + TRANSFORMED_BUCKETIZED_FEATURE_NAMES + TRANSFORMED_CATEGORICAL_FEATURE_NAMES:\n",
    "    slice_spec += [tfma.SingleSliceSpec(columns=[feature_name])]\n",
    "\n",
    "#print slice_spec\n",
    "    \n",
    "model_location = os.path.join(eval_model_dir, os.listdir(eval_model_dir)[0])\n",
    "data_location = Params.TRANSFORMED_EVAL_DATA_FILE_PREFIX+\"*.tfrecords\"\n",
    "\n",
    "eval_result = tfma.run_model_analysis(\n",
    "    model_location=model_location , \n",
    "    data_location=data_location, \n",
    "    file_format='tfrecords', \n",
    "    slice_spec=slice_spec, \n",
    "#     example_weight_key=None, \n",
    "#     output_path=None\n",
    ")\n",
    "\n",
    "# print eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Visualise TF Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(\n",
    "    result=eval_result, \n",
    "    slicing_column='job'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Predict Using the Serving Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls models/dnn_estimator/export/estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_base_dir=os.path.join(model_dir,'export/estimate')\n",
    "\n",
    "SAVED_MODEL_DIR=os.path.join(saved_model_base_dir, os.listdir(saved_model_base_dir)[0])\n",
    "\n",
    "def predict(instance):\n",
    " \n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "        export_dir=SAVED_MODEL_DIR,\n",
    "        signature_def_key=\"predict\"\n",
    "    )\n",
    "    \n",
    "    instance = dict((k, [v]) for k, v in instance.items())\n",
    "    value = predictor_fn(instance)\n",
    "    return value\n",
    "\n",
    "instance = {\n",
    "    \"age\":56,\n",
    "    \"job\":\"housemaid\",\n",
    "    \"marital_status\":\"married\",\n",
    "    \"education\":\"basic.4y\",\n",
    "    \"has_credit_default\":\"no\",\n",
    "    \"has_housing_loan\":\"no\",\n",
    "    \"has_personal_loan\":\"no\",\n",
    "    \"contact_type\":\"telephone\",\n",
    "    \"last_contact_month\":\"may\",\n",
    "    \"last_contact_day_of_week\":\"mon\",\n",
    "    \"last_contact_duration\":261,\n",
    "    \"campaign_contact_count\":1,\n",
    "    \"days_since_last_contact\":999,\n",
    "    \"previous_campaign_contact_count\":0,\n",
    "    \"previous_campaign_outcome\":\"nonexistent\",\n",
    "    \"employment_variation_rate\":1.1,\n",
    "    \"consumer_price_index\":93.994,\n",
    "    \"consumer_confidence_index\":-36.4,\n",
    "    \"euribor3m\":4.857,\n",
    "    \"number_of_employees\":5191.0,\n",
    "    #\"has_subscribed\":\"no\"\n",
    "}\n",
    "\n",
    "prediction = predict(instance)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have finished the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
