{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Babyweight Data Prepcessing with tf.Transfrom "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installed required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "pip install --ignore-installed tensorflow-transform==0.8.0\n",
    "pip install --upgrade dill==0.2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the session by clicking the bottom **Reset Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-gcp-playground' # change to your project_Id\n",
    "BUCKET = 'ksalama-gcs-cloudml' # change to your bucket name\n",
    "REGION = 'europe-west1' # change to your region\n",
    "ROOT_DIR = 'babyweight_tft' # directory where the output is stored locally or on GCS\n",
    "\n",
    "RUN_LOCAL = False # if True, the DirectRunner is used, else DataflowRunner\n",
    "DATA_SIZE = 10000 # number of records to be retrieved from BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['ROOT_DIR'] = ROOT_DIR\n",
    "os.environ['RUN_LOCAL'] = str(RUN_LOCAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "from tensorflow_transform.beam import impl\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.coders import example_proto_coder\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep 'tensorflow'\n",
    "!pip list | grep 'beam'\n",
    "!pip list | grep 'cloud-dataflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURE_NAMES = ['is_male', 'mother_race']\n",
    "NUMERIC_FEATURE_NAMES = ['mother_age', 'plurality', 'gestation_weeks']\n",
    "TARGET_FEATURE_NAME = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "def create_raw_metadata():  \n",
    "    \n",
    "    raw_data_schema = {}\n",
    "    \n",
    "    # key feature scehma\n",
    "    raw_data_schema[KEY_COLUMN]= dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "    \n",
    "    # target feature scehma\n",
    "    raw_data_schema[TARGET_FEATURE_NAME]= dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "    \n",
    "    # categorical features scehma\n",
    "    raw_data_schema.update({ column_name : dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in CATEGORICAL_FEATURE_NAMES})\n",
    "    \n",
    "    # numerical features scehma\n",
    "    raw_data_schema.update({ column_name : dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in NUMERIC_FEATURE_NAMES})\n",
    "    \n",
    "      # create dataset_metadata given raw_schema\n",
    "    raw_metadata = dataset_metadata.DatasetMetadata(\n",
    "        dataset_schema.Schema(raw_data_schema))\n",
    "    \n",
    "    return raw_metadata\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(create_raw_metadata().schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_query(step, data_size):\n",
    "    \n",
    "    train_size = data_size * 0.7\n",
    "    eval_size = data_size * 0.3\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      ROUND(weight_pounds,1) AS weight_pounds,\n",
    "      is_male,\n",
    "      mother_age,\n",
    "      mother_race,\n",
    "      plurality,\n",
    "      gestation_weeks,\n",
    "      FARM_FINGERPRINT( \n",
    "        CONCAT(\n",
    "          COALESCE(CAST(weight_pounds AS STRING), 'NA'),\n",
    "          COALESCE(CAST(is_male AS STRING),'NA'),\n",
    "          COALESCE(CAST(mother_age AS STRING),'NA'),\n",
    "          COALESCE(CAST(mother_race AS STRING),'NA'),\n",
    "          COALESCE(CAST(plurality AS STRING), 'NA'),\n",
    "          COALESCE(CAST(gestation_weeks AS STRING),'NA')\n",
    "          )\n",
    "        ) AS key\n",
    "        FROM\n",
    "          publicdata.samples.natality\n",
    "        WHERE year > 2000\n",
    "        AND weight_pounds > 0\n",
    "        AND mother_age > 0\n",
    "        AND plurality > 0\n",
    "        AND gestation_weeks > 0\n",
    "        AND month > 0\n",
    "    \"\"\"\n",
    "    \n",
    "    if step == 'train':\n",
    "        source_query = 'SELECT * FROM ({}) WHERE MOD(key, 100) < 70 LIMIT {}'.format(query,int(train_size))\n",
    "    else:\n",
    "        source_query = 'SELECT * FROM ({}) WHERE MOD(key, 100) >= 70 LIMIT {}'.format(query,int(eval_size))\n",
    "    \n",
    "    return source_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source cleanup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_bq_row(bq_row):\n",
    "\n",
    "    # modify opaque numeric race code into human-readable data\n",
    "    races = dict(zip([1,2,3,4,5,6,7,18,28,39,48],\n",
    "                     ['White', 'Black', 'American Indian', 'Chinese', \n",
    "                      'Japanese', 'Hawaiian', 'Filipino',\n",
    "                      'Asian Indian', 'Korean', 'Samaon', 'Vietnamese']))\n",
    "    result = {} \n",
    "    \n",
    "    for feature_name in bq_row.keys():\n",
    "        result[feature_name] = str(bq_row[feature_name])\n",
    "\n",
    "    if 'mother_race' in bq_row and bq_row['mother_race'] in races:\n",
    "        result['mother_race'] = races[bq_row['mother_race']]\n",
    "    else:\n",
    "        result['mother_race'] = 'Unknown'\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and clean from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_bq(pipeline, step, data_size):\n",
    "    \n",
    "    source_query = get_source_query(step, data_size)\n",
    "    raw_data = (\n",
    "        pipeline\n",
    "        | '{} - Read Data from BigQuery'.format(step) >> beam.io.Read(\n",
    "            beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "        | '{} - Clean up Data'.format(step) >> beam.Map(prep_bq_row)\n",
    "    )\n",
    "    \n",
    "    raw_metadata = create_raw_metadata()\n",
    "    raw_dataset = (raw_data, raw_metadata)\n",
    "    return raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.Transform preprocess_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(input_features):\n",
    "    \n",
    "    output_features = {}\n",
    "\n",
    "    # target feature\n",
    "    output_features['weight_pounds'] = input_features['weight_pounds']\n",
    "\n",
    "    # normalisation\n",
    "    output_features['mother_age_normalized'] = tft.scale_to_z_score(input_features['mother_age'])\n",
    "    output_features['gestation_weeks_normalized'] =  tft.scale_to_0_1(input_features['gestation_weeks'])\n",
    "    \n",
    "    # bucktisation based on quantiles\n",
    "    output_features['mother_age_bucketized'] = tft.bucketize(input_features['mother_age'], num_buckets=5)\n",
    "    \n",
    "    # you can compute new features based on custom formulas\n",
    "    output_features['mother_age_log'] = tf.log(input_features['mother_age'])\n",
    "    \n",
    "    # or create flags/indicators\n",
    "    is_multiple = tf.as_string(input_features['plurality'] > tf.constant(1.0))\n",
    "    \n",
    "    # convert categorical features to indexed vocab\n",
    "    output_features['mother_race_index'] = tft.compute_and_apply_vocabulary(input_features['mother_race'], vocab_filename='mother_race')\n",
    "    output_features['is_male_index'] = tft.compute_and_apply_vocabulary(input_features['is_male'], vocab_filename='is_male')\n",
    "    output_features['is_multiple_index'] = tft.compute_and_apply_vocabulary(is_multiple, vocab_filename='is_multiple')\n",
    "    \n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_transform(raw_dataset, step):\n",
    "    \n",
    "    transformed_dataset, transform_fn = (\n",
    "        raw_dataset \n",
    "        | '{} - Analyze & Transform'.format(step) >> impl.AnalyzeAndTransformDataset(preprocess_fn)\n",
    "    )\n",
    "    \n",
    "    return transformed_dataset, transform_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_dataset, transform_fn, step):\n",
    "    \n",
    "    transformed_dataset = (\n",
    "        (raw_dataset, transform_fn) \n",
    "        | '{} - Transform'.format(step) >> impl.TransformDataset()\n",
    "    )\n",
    "    \n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecords(transformed_dataset, location, step):\n",
    "    \n",
    "    transformed_data, transformed_metadata = transformed_dataset\n",
    "    (\n",
    "        transformed_data \n",
    "        | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "            file_path_prefix=os.path.join(location,'{}'.format(step)),\n",
    "            file_name_suffix=\".tfrecords\",\n",
    "            coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text(dataset, location, step):\n",
    "    \n",
    "    data, _ = dataset\n",
    "    (\n",
    "        data \n",
    "        | '{} - WriteData'.format(step) >> beam.io.WriteToText(\n",
    "            file_path_prefix=os.path.join(location,'{}'.format(step)),\n",
    "            file_name_suffix=\".txt\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_transform_artefacts(transform_fn, location):\n",
    "    \n",
    "    (\n",
    "        transform_fn \n",
    "        | 'Write Transform Artefacts' >> transform_fn_io.WriteTransformFn(location)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transformation_pipeline(args):\n",
    "    \n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "    \n",
    "    runner = args['runner']\n",
    "    data_size = args['data_size']\n",
    "    transformed_data_location = args['transformed_data_location']\n",
    "    transform_artefact_location = args['transform_artefact_location']\n",
    "    temporary_dir = args['temporary_dir']\n",
    "    debug = args['debug']\n",
    "    \n",
    "    print(\"Sample data size: {}\".format(data_size))\n",
    "    print(\"Sink transformed data files location: {}\".format(transformed_data_location))\n",
    "    print(\"Sink transform artefact location: {}\".format(transform_artefact_location))\n",
    "    print(\"Temporary directory: {}\".format(temporary_dir))\n",
    "    print(\"Runner: {}\".format(runner))\n",
    "    print(\"Debug enabled: {}\".format(debug))\n",
    "\n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "        with impl.Context(temporary_dir):\n",
    "            \n",
    "            # Preprocess train data\n",
    "            step = 'train'\n",
    "            # Read raw train data from BQ\n",
    "            raw_train_dataset = read_from_bq(pipeline, step, data_size)\n",
    "            # Analyze and transform raw_train_dataset \n",
    "            transformed_train_dataset, transform_fn = analyze_and_transform(raw_train_dataset, step)\n",
    "            # Write transformed train data to sink as tfrecords\n",
    "            write_tfrecords(transformed_train_dataset, transformed_data_location, step)\n",
    "            \n",
    "            # Preprocess evaluation data\n",
    "            step = 'eval'\n",
    "            # Read raw eval data from BQ\n",
    "            raw_eval_dataset = read_from_bq(pipeline, step, data_size)\n",
    "            # Transform eval data based on produced transform_fn\n",
    "            transformed_eval_dataset = transform(raw_eval_dataset, transform_fn, step)\n",
    "            # Write transformed eval data to sink as tfrecords\n",
    "            write_tfrecords(transformed_eval_dataset, transformed_data_location, step)\n",
    "            \n",
    "            # Write transformation artefacts \n",
    "            write_transform_artefacts(transform_fn, transform_artefact_location)\n",
    "\n",
    "            # (Optional) for debugging, write transformed data as text \n",
    "            step = 'debug'\n",
    "            # Wwrite transformed train data as text if debug enabled\n",
    "            if debug == True:\n",
    "                write_text(transformed_train_dataset, transformed_data_location, step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set pipeline params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "tensorflow-transform==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "OUTPUT_DIR = ROOT_DIR if RUN_LOCAL==True else \"gs://{}/{}\".format(BUCKET,ROOT_DIR)\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(OUTPUT_DIR,'transform')\n",
    "TRANSFORMED_DATA_DIR = os.path.join(OUTPUT_DIR,'transformed')\n",
    "TEMP_DIR = os.path.join(OUTPUT_DIR, 'tmp')\n",
    "\n",
    "runner = 'DirectRunner' if RUN_LOCAL == True else 'DataflowRunner'\n",
    "\n",
    "job_name = 'preprocess-babweight-data-tft-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "args = {\n",
    "    \n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'data_size': DATA_SIZE,\n",
    "    'transformed_data_location':  TRANSFORMED_DATA_DIR,\n",
    "    'transform_artefact_location':  TRANSFORM_ARTEFACTS_DIR,\n",
    "    'temporary_dir': TEMP_DIR,\n",
    "    'debug':False,\n",
    "    \n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'worker_machine_type': 'n1-standard-1',\n",
    "    'requirements_file': 'requirements.txt',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    tf.gfile.DeleteRecursively(TRANSFORMED_DATA_DIR)\n",
    "    tf.gfile.DeleteRecursively(TRANSFORM_ARTEFACTS_DIR)\n",
    "    tf.gfile.DeleteRecursively(TEMP_DIR)\n",
    "    print 'previous transformation files deleted!'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "print 'Launching {} job {} ... hang on'.format(runner, job_name)\n",
    "print(\"\")\n",
    "run_transformation_pipeline(args)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the produced artefacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo 'local run:' ${RUN_LOCAL}\n",
    "echo 'directory:' ${ROOT_DIR}\n",
    "echo ''\n",
    "\n",
    "echo 'transformed data:' \n",
    "if [ \"${RUN_LOCAL}\" = \"True \" ] \n",
    "then ls ${ROOT_DIR}/transformed \n",
    "else gsutil ls gs://${BUCKET}/${ROOT_DIR}/transformed \n",
    "fi\n",
    "echo ''\n",
    "\n",
    "echo 'transformed metadata:'  \n",
    "if [ \"${RUN_LOCAL}\" = \"True \" ] \n",
    "then ls ${ROOT_DIR}/transform/transformed_metadata\n",
    "else gsutil ls gs://${BUCKET}/${ROOT_DIR}/transform/transformed_metadata \n",
    "fi\n",
    "echo ''\n",
    "\n",
    "echo 'transform artefact:'   \n",
    "if [ \"${RUN_LOCAL}\" = \"True \" ] \n",
    "then ls ${ROOT_DIR}/transform/transform_fn\n",
    "else gsutil ls gs://${BUCKET}/${ROOT_DIR}/transform/transform_fn \n",
    "fi\n",
    "echo ''\n",
    "\n",
    "echo 'transform assets:'\n",
    "if [ \"${RUN_LOCAL}\" = \"True \" ] \n",
    "then ls ${ROOT_DIR}/transform/transform_fn/assets\n",
    "else gsutil ls gs://${BUCKET}/${ROOT_DIR}/transform/transform_fn/assets \n",
    "fi\n",
    "echo ''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
