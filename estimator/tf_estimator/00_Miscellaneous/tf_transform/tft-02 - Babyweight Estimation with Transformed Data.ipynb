{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Babyweight Estimation with Transformed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-gcp-playground' # change to your project_Id\n",
    "BUCKET = 'ksalama-gcs-cloudml' # change to your bucket name\n",
    "REGION = 'europe-west1' # change to your region\n",
    "ROOT_DIR = 'babyweight_tft' # directory where the output is stored locally or on GCS\n",
    "\n",
    "RUN_LOCAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['ROOT_DIR'] = ROOT_DIR\n",
    "os.environ['RUN_LOCAL'] = 'true' if RUN_LOCAL else 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep 'tensorflow'\n",
    "!pip list | grep 'beam'\n",
    "!pip list | grep 'cloud-dataflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = ROOT_DIR if RUN_LOCAL==True else \"gs://{}/{}\".format(BUCKET,ROOT_DIR)\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(OUTPUT_DIR,'transform')\n",
    "TRANSFORMED_DATA_DIR = os.path.join(OUTPUT_DIR,'transformed')\n",
    "TEMP_DIR = os.path.join(OUTPUT_DIR, 'tmp')\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR,'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_metadata = metadata_io.read_metadata(\n",
    "        os.path.join(TRANSFORM_ARTEFACTS_DIR,\"transformed_metadata\"))\n",
    "\n",
    "TARGET_FEATURE_NAME = 'weight_pounds'\n",
    "\n",
    "print transformed_metadata.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecords_input_fn(files_name_pattern, transformed_metadata,\n",
    "                       mode=tf.estimator.ModeKeys.EVAL,  \n",
    "                       num_epochs=1, \n",
    "                       batch_size=500):\n",
    "    \n",
    "    dataset = tf.contrib.data.make_batched_features_dataset(\n",
    "        file_pattern=files_name_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_metadata.schema.as_feature_spec(),\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=True if mode == tf.estimator.ModeKeys.TRAIN else False,\n",
    "        shuffle_buffer_size=1+(batch_size*2),\n",
    "        prefetch_buffer_size=1\n",
    "    )\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features = iterator.get_next()\n",
    "    target = features.pop(TARGET_FEATURE_NAME)\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_wide_and_deep_feature_columns(transformed_metadata, hparams):\n",
    "    \n",
    "    deep_feature_columns = []\n",
    "    wide_feature_columns = []\n",
    "    \n",
    "    column_schemas = transformed_metadata.schema.column_schemas\n",
    "    \n",
    "    for feature_name in column_schemas:\n",
    "        if feature_name == TARGET_FEATURE_NAME:\n",
    "            continue\n",
    "        column_schema = column_schemas[feature_name]\n",
    "        \n",
    "        # creating numerical features\n",
    "        if isinstance(column_schema._domain, dataset_schema.FloatDomain):\n",
    "            deep_feature_columns.append(tf.feature_column.numeric_column(feature_name))\n",
    "            \n",
    "        # creating categorical features with identity\n",
    "        elif isinstance(column_schema._domain, dataset_schema.IntDomain):\n",
    "            if column_schema._domain._is_categorical==True:\n",
    "                wide_feature_columns.append(\n",
    "                    tf.feature_column.categorical_column_with_identity(\n",
    "                        feature_name, \n",
    "                        num_buckets=column_schema._domain._max_value+1)\n",
    "                )\n",
    "            else:\n",
    "                deep_feature_columns.append(tf.feature_column.numeric_column(feature_name)) \n",
    "     \n",
    "    if hparams.extend_feature_columns==True:\n",
    "        mother_race_X_mother_age_bucketized = tf.feature_column.crossed_column(\n",
    "            ['mother_age_bucketized', 'mother_race_index'],  55)\n",
    "        \n",
    "        wide_feature_columns.append(mother_race_X_mother_age_bucketized)\n",
    "        \n",
    "        mother_race_X_mother_age_bucketized_embedded = tf.feature_column.embedding_column(\n",
    "            mother_race_X_mother_age_bucketized, hparams.embed_dimension)\n",
    "        \n",
    "        deep_feature_columns.append(mother_race_X_mother_age_bucketized_embedded)\n",
    "    \n",
    "    print \"Wide columns:\"\n",
    "    print wide_feature_columns\n",
    "    print \"\"\n",
    "    print \"Deep columns:\"\n",
    "    print deep_feature_columns\n",
    "    print \"\"\n",
    "    \n",
    "    return wide_feature_columns, deep_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(run_config, hparams):\n",
    "  \n",
    "    wide_feature_columns, deep_feature_columns = create_wide_and_deep_feature_columns(transformed_metadata, \n",
    "                                                                                      hparams)\n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "                linear_feature_columns = wide_feature_columns,\n",
    "                dnn_feature_columns = deep_feature_columns,\n",
    "                dnn_hidden_units=hparams.hidden_units,\n",
    "                config = run_config\n",
    "                )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs=10,\n",
    "    batch_size=500,\n",
    "    hidden_units=[32, 16],\n",
    "    max_steps=100,\n",
    "    embed_dimension=5,\n",
    "    extend_feature_columns=False,\n",
    "    evaluate_after_sec=10\n",
    ")\n",
    "\n",
    "model_dir = os.path.join(MODELS_DIR,\"dnn_estimator\")\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    model_dir=model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_files = os.path.join(TRANSFORMED_DATA_DIR, \"train-*.tfrecords\")\n",
    "eval_data_files = os.path.join(TRANSFORMED_DATA_DIR, \"eval-*.tfrecords\")\n",
    "\n",
    "# TrainSpec\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "  input_fn = lambda: tfrecords_input_fn(train_data_files,transformed_metadata,\n",
    "    mode=tf.estimator.ModeKeys.TRAIN,\n",
    "    num_epochs= hparams.num_epochs,\n",
    "    batch_size = hparams.batch_size\n",
    "  ),\n",
    "  max_steps=hparams.max_steps,\n",
    ")\n",
    "\n",
    "# EvalSpec\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "  input_fn =lambda: tfrecords_input_fn(eval_data_files,transformed_metadata),\n",
    "  steps = None,\n",
    "  throttle_secs = hparams.evaluate_after_sec # evalute after each 10 training seconds!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if tf.gfile.Exists(model_dir):\n",
    "    tf.gfile.DeleteRecursively(model_dir)\n",
    "\n",
    "estimator = create_estimator(run_config, hparams)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow() \n",
    "print(\"\")\n",
    "print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\") \n",
    "\n",
    "\n",
    "tf.estimator.train_and_evaluate(\n",
    "  estimator,\n",
    "  train_spec,\n",
    "  eval_spec\n",
    ")\n",
    "\n",
    "\n",
    "time_end = datetime.utcnow() \n",
    "print(\".......................................\")\n",
    "print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURE_NAMES = ['is_male', 'mother_race']\n",
    "NUMERIC_FEATURE_NAMES = ['mother_age', 'plurality', 'gestation_weeks']\n",
    "TARGET_FEATURE_NAME = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "def create_raw_metadata():  \n",
    "    \n",
    "    raw_data_schema = {}\n",
    "    \n",
    "    # key feature scehma\n",
    "    raw_data_schema[KEY_COLUMN]= dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "    \n",
    "    # target feature scehma\n",
    "    raw_data_schema[TARGET_FEATURE_NAME]= dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "    \n",
    "    # categorical features scehma\n",
    "    raw_data_schema.update({ column_name : dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in CATEGORICAL_FEATURE_NAMES})\n",
    "    \n",
    "    # numerical features scehma\n",
    "    raw_data_schema.update({ column_name : dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in NUMERIC_FEATURE_NAMES})\n",
    "    \n",
    "      # create dataset_metadata given raw_schema\n",
    "    raw_metadata = dataset_metadata.DatasetMetadata(\n",
    "        dataset_schema.Schema(raw_data_schema))\n",
    "    \n",
    "    return raw_metadata\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(create_raw_metadata().schema.as_feature_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Estimator to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \n",
    "    from tensorflow_transform.saved import saved_transform_io\n",
    "    \n",
    "    # get the feature_spec of raw data\n",
    "    raw_metadata = create_raw_metadata()\n",
    "    \n",
    "    # create receiver placeholders to the raw input features\n",
    "    raw_input_features = raw_metadata.schema.as_batched_placeholders()\n",
    "    raw_input_features.pop(TARGET_FEATURE_NAME)\n",
    "    raw_input_features.pop(KEY_COLUMN)\n",
    "\n",
    "    # apply tranform_fn on raw features\n",
    "    _, transformed_features = (\n",
    "        saved_transform_io.partially_apply_saved_transform(\n",
    "            os.path.join(TRANSFORM_ARTEFACTS_DIR,transform_fn_io.TRANSFORM_FN_DIR),\n",
    "        raw_input_features)\n",
    "    )\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        transformed_features, raw_input_features)\n",
    "\n",
    "export_dir = os.path.join(model_dir, 'export')\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "    tf.gfile.DeleteRecursively(export_dir)\n",
    "        \n",
    "estimator.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=serving_input_receiver_fn\n",
    ")\n",
    "\n",
    "os.environ['export_dir'] = export_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ${RUN_LOCAL} ]\n",
    "then \n",
    "saved_model_dir=$(gsutil ls ${export_dir} | tail -n 1)\n",
    "else\n",
    "saved_model_dir=${export_dir}/$(ls ${export_dir} | tail -n 1)\n",
    "fi\n",
    "\n",
    "echo $saved_model_dir\n",
    "saved_model_cli show --dir=${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Exported Model for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir=os.path.join(export_dir, tf.gfile.ListDirectory(export_dir)[0])\n",
    "\n",
    "print saved_model_dir\n",
    "\n",
    "def estimate_local(instance):\n",
    " \n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "        export_dir=saved_model_dir,\n",
    "        signature_def_key=\"predict\"\n",
    "    )\n",
    "    \n",
    "    instance = dict((k, [v]) for k, v in instance.items())\n",
    "    value = predictor_fn(instance)['predictions'][0][0]\n",
    "    return value\n",
    "\n",
    "instance = {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39\n",
    "}\n",
    "\n",
    "prediction = estimate_local(instance)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
