{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalidsalama/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import math\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the TF Estimator APIs\n",
    "1. Define dataset **metadata**\n",
    "2. Define **data input function** to read the data from Pandas dataframe + **apply feature processing**\n",
    "3. Create TF **feature columns** based on metadata + **extended feature columns**\n",
    "4. Instantiate an **estimator** with the required **feature columns & parameters**\n",
    "5. **Train** estimator using training data\n",
    "6. **Evaluate** estimator using test data\n",
    "7.  Perform **predictions**\n",
    "8. **Save & Serve** the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'reg-model-01'\n",
    "\n",
    "TRAIN_DATA_FILE = 'data/train-data.csv'\n",
    "VALID_DATA_FILE = 'data/valid-data.csv'\n",
    "TEST_DATA_FILE = 'data/test-data.csv'\n",
    "\n",
    "RESUME_TRAINING = False\n",
    "PROCESS_FEATURES = True\n",
    "MULTI_THREADING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Dataset Metadata\n",
    "* CSV file header and defaults\n",
    "* Numeric and categorical feature names\n",
    "* Target feature name\n",
    "* Unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['key', 'x', 'y', 'alpha', 'beta', 'target']\n",
      "Numeric Features: ['x', 'y']\n",
      "Categorical Features: ['alpha', 'beta']\n",
      "Target: target\n",
      "Unused Features: ['key']\n"
     ]
    }
   ],
   "source": [
    "HEADER = ['key','x','y','alpha','beta','target']\n",
    "HEADER_DEFAULTS = [[0], [0.0], [0.0], ['NA'], ['NA'], [0.0]]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['x', 'y']  \n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY = {'alpha':['ax01', 'ax02'], 'beta':['bx01', 'bx02']}\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "TARGET_NAME = 'target'\n",
    "\n",
    "UNUSED_FEATURE_NAMES = list(set(HEADER) - set(FEATURE_NAMES) - {TARGET_NAME})\n",
    "\n",
    "print(\"Header: {}\".format(HEADER))\n",
    "print(\"Numeric Features: {}\".format(NUMERIC_FEATURE_NAMES))\n",
    "print(\"Categorical Features: {}\".format(CATEGORICAL_FEATURE_NAMES))\n",
    "print(\"Target: {}\".format(TARGET_NAME))\n",
    "print(\"Unused Features: {}\".format(UNUSED_FEATURE_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Data Input Function\n",
    "* Input csv file name\n",
    "* Load pandas Dataframe\n",
    "* Apply feature processing\n",
    "* Return a function that returns (features, target) tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_dataframe(dataset_df):\n",
    "    \n",
    "    dataset_df[\"x_2\"] = np.square(dataset_df['x'])\n",
    "    dataset_df[\"y_2\"] = np.square(dataset_df['y'])\n",
    "    dataset_df[\"xy\"] = dataset_df['x'] * dataset_df['y']\n",
    "    dataset_df['dist_xy'] =  np.sqrt(np.square(dataset_df['x']-dataset_df['y']))\n",
    "    \n",
    "    return dataset_df\n",
    "\n",
    "def generate_pandas_input_fn(file_name, mode=tf.estimator.ModeKeys.EVAL,\n",
    "                             skip_header_lines=0,\n",
    "                             num_epochs=1,\n",
    "                             batch_size=100):\n",
    "\n",
    "    df_dataset = pd.read_csv(file_name, names=HEADER, skiprows=skip_header_lines)\n",
    "    \n",
    "    x = df_dataset[FEATURE_NAMES].copy()\n",
    "    if PROCESS_FEATURES:\n",
    "        x = process_dataframe(x)\n",
    "    \n",
    "    y = df_dataset[TARGET_NAME]\n",
    "        \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    num_threads=1\n",
    "    \n",
    "    if MULTI_THREADING:\n",
    "        num_threads=multiprocessing.cpu_count()\n",
    "        num_epochs = int(num_epochs/num_threads) if mode == tf.estimator.ModeKeys.TRAIN else num_epochs\n",
    "    \n",
    "    pandas_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        batch_size=batch_size,\n",
    "        num_epochs= num_epochs,\n",
    "        shuffle=shuffle,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        target_column=TARGET_NAME\n",
    "    )\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"* data input_fn:\")\n",
    "    print(\"================\")\n",
    "    print(\"Input file: {}\".format(file_name))\n",
    "    print(\"Dataset size: {}\".format(len(df_dataset)))\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    print(\"Epoch Count: {}\".format(num_epochs))\n",
    "    print(\"Mode: {}\".format(mode))\n",
    "    print(\"Thread Count: {}\".format(num_threads))\n",
    "    print(\"Shuffle: {}\".format(shuffle))\n",
    "    print(\"================\")\n",
    "    print(\"\")\n",
    "    \n",
    "    return pandas_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/train-data.csv\n",
      "Dataset size: 12000\n",
      "Batch size: 100\n",
      "Epoch Count: 1\n",
      "Mode: eval\n",
      "Thread Count: 1\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "Feature read from DataFrame: ['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']\n",
      "Target read from DataFrame: Tensor(\"fifo_queue_DequeueUpTo:9\", shape=(?,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "features, target = generate_pandas_input_fn(file_name=TRAIN_DATA_FILE)()\n",
    "print(\"Feature read from DataFrame: {}\".format(list(features.keys())))\n",
    "print(\"Target read from DataFrame: {}\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Feature Columns\n",
    "The input numeric columns are assumed to be normalized (or have the same scale). Otherwise, a normlizer_fn, along with the normlisation params (mean, stdv or min, max) should be passed to tf.feature_column.numeric_column() constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: {'x': _NumericColumn(key='x', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'y': _NumericColumn(key='y', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'x_2': _NumericColumn(key='x_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'y_2': _NumericColumn(key='y_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'xy': _NumericColumn(key='xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'dist_xy': _NumericColumn(key='dist_xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'alpha': _VocabularyListCategoricalColumn(key='alpha', vocabulary_list=('ax01', 'ax02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'beta': _VocabularyListCategoricalColumn(key='beta', vocabulary_list=('bx01', 'bx02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'alpha_X_beta': _CrossedColumn(keys=(_VocabularyListCategoricalColumn(key='alpha', vocabulary_list=('ax01', 'ax02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), _VocabularyListCategoricalColumn(key='beta', vocabulary_list=('bx01', 'bx02'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), hash_bucket_size=4, hash_key=None)}\n"
     ]
    }
   ],
   "source": [
    "def get_feature_columns():\n",
    "    \n",
    "    \n",
    "    all_numeric_feature_names = NUMERIC_FEATURE_NAMES\n",
    "    \n",
    "    CONSTRUCTED_NUMERIC_FEATURES_NAMES = ['x_2', 'y_2', 'xy', 'dist_xy']\n",
    "    \n",
    "    if PROCESS_FEATURES:\n",
    "        all_numeric_feature_names += CONSTRUCTED_NUMERIC_FEATURES_NAMES\n",
    "\n",
    "    numeric_columns = {feature_name: tf.feature_column.numeric_column(feature_name)\n",
    "                       for feature_name in all_numeric_feature_names}\n",
    "\n",
    "    categorical_column_with_vocabulary = \\\n",
    "        {item[0]: tf.feature_column.categorical_column_with_vocabulary_list(item[0], item[1])\n",
    "         for item in CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.items()}\n",
    "        \n",
    "    feature_columns = {}\n",
    "\n",
    "    if numeric_columns is not None:\n",
    "        feature_columns.update(numeric_columns)\n",
    "\n",
    "    if categorical_column_with_vocabulary is not None:\n",
    "        feature_columns.update(categorical_column_with_vocabulary)\n",
    "        \n",
    "    # add extended features (crossing, bucektization, embedding)\n",
    "    \n",
    "    feature_columns['alpha_X_beta'] = tf.feature_column.crossed_column(\n",
    "        [feature_columns['alpha'], feature_columns['beta']], 4)\n",
    "    \n",
    "    return feature_columns\n",
    "\n",
    "feature_columns = get_feature_columns()\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create an Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Define an Estimator Creation Function\n",
    "\n",
    "* Get dense (numeric) columns from the feature columns\n",
    "* Convert categorical columns to indicator columns\n",
    "* Create Instantiate a DNNRegressor estimator given **dense + indicator** feature columns + params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_estimator(run_config, hparams):\n",
    "    \n",
    "    feature_columns = list(get_feature_columns().values())\n",
    "    \n",
    "    dense_columns = list(\n",
    "        filter(lambda column: isinstance(column, feature_column._NumericColumn),\n",
    "               feature_columns\n",
    "        )\n",
    "    )\n",
    "\n",
    "    categorical_columns = list(\n",
    "        filter(lambda column: isinstance(column, feature_column._VocabularyListCategoricalColumn) |\n",
    "                              isinstance(column, feature_column._BucketizedColumn),\n",
    "                   feature_columns)\n",
    "    )\n",
    "\n",
    "    indicator_columns = list(\n",
    "            map(lambda column: tf.feature_column.indicator_column(column),\n",
    "                categorical_columns)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    estimator_feature_columns = dense_columns + indicator_columns \n",
    "    \n",
    "    estimator = tf.estimator.DNNRegressor(\n",
    "        \n",
    "        feature_columns= estimator_feature_columns,\n",
    "        hidden_units= hparams.hidden_units,\n",
    "        \n",
    "        optimizer= tf.train.AdamOptimizer(),\n",
    "        activation_fn= tf.nn.elu,\n",
    "        dropout= hparams.dropout_prob,\n",
    "        \n",
    "        config= run_config\n",
    "    )\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Estimator Type: {}\".format(type(estimator)))\n",
    "    print(\"\")\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Set hyper-parameter values (HParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: trained_models/reg-model-01\n",
      "Hyper-parameters: [('batch_size', 500), ('dropout_prob', 0.0), ('hidden_units', [8, 4]), ('num_epochs', 100)]\n"
     ]
    }
   ],
   "source": [
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs = 100,\n",
    "    batch_size = 500,\n",
    "    hidden_units=[8, 4], \n",
    "    dropout_prob = 0.0)\n",
    "\n",
    "\n",
    "model_dir = 'trained_models/{}'.format(MODEL_NAME)\n",
    "\n",
    "run_config = tf.estimator.RunConfig().replace(model_dir=model_dir)\n",
    "print(\"Model directory: {}\".format(run_config.model_dir))\n",
    "print(\"Hyper-parameters: {}\".format(hparams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Instantiate the estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_models/reg-model-01', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1280145f8>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "Estimator Type: <class 'tensorflow.python.estimator.canned.dnn.DNNRegressor'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator = create_estimator(run_config, hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/train-data.csv\n",
      "Dataset size: 12000\n",
      "Batch size: 500\n",
      "Epoch Count: 100\n",
      "Mode: train\n",
      "Thread Count: 1\n",
      "Shuffle: True\n",
      "================\n",
      "\n",
      "Estimator training started at 19:19:12\n",
      ".......................................\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into trained_models/reg-model-01/model.ckpt.\n",
      "INFO:tensorflow:loss = 179225.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 166.515\n",
      "INFO:tensorflow:loss = 124778.0, step = 101 (0.602 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.042\n",
      "INFO:tensorflow:loss = 144432.0, step = 201 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.401\n",
      "INFO:tensorflow:loss = 167542.0, step = 301 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.414\n",
      "INFO:tensorflow:loss = 146349.0, step = 401 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.184\n",
      "INFO:tensorflow:loss = 148680.0, step = 501 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.155\n",
      "INFO:tensorflow:loss = 123907.0, step = 601 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.701\n",
      "INFO:tensorflow:loss = 113046.0, step = 701 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.637\n",
      "INFO:tensorflow:loss = 107878.0, step = 801 (0.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.787\n",
      "INFO:tensorflow:loss = 118305.0, step = 901 (0.788 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.261\n",
      "INFO:tensorflow:loss = 101507.0, step = 1001 (0.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.629\n",
      "INFO:tensorflow:loss = 106166.0, step = 1101 (0.616 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.706\n",
      "INFO:tensorflow:loss = 107934.0, step = 1201 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.23\n",
      "INFO:tensorflow:loss = 98094.9, step = 1301 (0.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.572\n",
      "INFO:tensorflow:loss = 89144.2, step = 1401 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.678\n",
      "INFO:tensorflow:loss = 104465.0, step = 1501 (0.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.081\n",
      "INFO:tensorflow:loss = 92220.2, step = 1601 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.108\n",
      "INFO:tensorflow:loss = 79086.9, step = 1701 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.97\n",
      "INFO:tensorflow:loss = 93577.3, step = 1801 (0.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.418\n",
      "INFO:tensorflow:loss = 75269.3, step = 1901 (0.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.944\n",
      "INFO:tensorflow:loss = 73518.7, step = 2001 (0.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.012\n",
      "INFO:tensorflow:loss = 75916.3, step = 2101 (0.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.054\n",
      "INFO:tensorflow:loss = 65138.1, step = 2201 (0.768 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.839\n",
      "INFO:tensorflow:loss = 65868.5, step = 2301 (0.777 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2400 into trained_models/reg-model-01/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 88071.1.\n",
      ".......................................\n",
      "Estimator training finished at 19:19:30\n",
      "\n",
      "Estimator training elapsed time: 17.686301 seconds\n"
     ]
    }
   ],
   "source": [
    "train_input_fn = generate_pandas_input_fn(file_name= TRAIN_DATA_FILE, \n",
    "                                      mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                                      num_epochs=hparams.num_epochs,\n",
    "                                      batch_size=hparams.batch_size) \n",
    "\n",
    "if not RESUME_TRAINING:\n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "    \n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow() \n",
    "print(\"Estimator training started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\")\n",
    "\n",
    "estimator.train(input_fn = train_input_fn)\n",
    "\n",
    "time_end = datetime.utcnow() \n",
    "print(\".......................................\")\n",
    "print(\"Estimator training finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Estimator training elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/test-data.csv\n",
      "Dataset size: 5000\n",
      "Batch size: 5000\n",
      "Epoch Count: 1\n",
      "Mode: eval\n",
      "Thread Count: 1\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-14-19:19:30\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01/model.ckpt-2400\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-14-19:19:31\n",
      "INFO:tensorflow:Saving dict for global step 2400: average_loss = 164.862, global_step = 2400, loss = 824311.0\n",
      "\n",
      "{'average_loss': 164.86218, 'loss': 824310.88, 'global_step': 2400}\n",
      "\n",
      "RMSE: 12.83987\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 5000\n",
    "\n",
    "test_input_fn = generate_pandas_input_fn(file_name=TEST_DATA_FILE, \n",
    "                                      mode= tf.estimator.ModeKeys.EVAL,\n",
    "                                      batch_size= TEST_SIZE)\n",
    "\n",
    "results = estimator.evaluate(input_fn=test_input_fn)\n",
    "print(\"\")\n",
    "print(results)\n",
    "rmse = round(math.sqrt(results[\"average_loss\"]),5)\n",
    "print(\"\")\n",
    "print(\"RMSE: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/test-data.csv\n",
      "Dataset size: 5000\n",
      "Batch size: 5\n",
      "Epoch Count: 1\n",
      "Mode: infer\n",
      "Thread Count: 1\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01/model.ckpt-2400\n",
      "\n",
      "Predicted Values: [13.141397, -5.9562521, 11.541443, 3.8178449, 2.1242597]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "predict_input_fn = generate_pandas_input_fn(file_name=TEST_DATA_FILE, \n",
    "                                      mode= tf.estimator.ModeKeys.PREDICT,\n",
    "                                      batch_size= 5)\n",
    "\n",
    "predictions = estimator.predict(input_fn=predict_input_fn)\n",
    "values = list(map(lambda item: item[\"predictions\"][0],list(itertools.islice(predictions, 5))))\n",
    "print()\n",
    "print(\"Predicted Values: {}\".format(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save & Serve the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Define Seving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_features(features):\n",
    "    \n",
    "    features[\"x_2\"] = tf.square(features['x'])\n",
    "    features[\"y_2\"] = tf.square(features['y'])\n",
    "    features[\"xy\"] = tf.multiply(features['x'], features['y'])\n",
    "    features['dist_xy'] =  tf.sqrt(tf.squared_difference(features['x'],features['y']))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def csv_serving_input_fn():\n",
    "    \n",
    "    SERVING_HEADER = ['x','y','alpha','beta']\n",
    "    SERVING_HEADER_DEFAULTS = [[0.0], [0.0], ['NA'], ['NA']]\n",
    "\n",
    "    rows_string_tensor = tf.placeholder(dtype=tf.string,\n",
    "                                         shape=[None],\n",
    "                                         name='csv_rows')\n",
    "    \n",
    "    receiver_tensor = {'csv_rows': rows_string_tensor}\n",
    "\n",
    "    row_columns = tf.expand_dims(rows_string_tensor, -1)\n",
    "    columns = tf.decode_csv(row_columns, record_defaults=SERVING_HEADER_DEFAULTS)\n",
    "    features = dict(zip(SERVING_HEADER, columns))\n",
    "    \n",
    "    if PROCESS_FEATURES:\n",
    "        features = process_features(features)\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Export SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01/model.ckpt-2400\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"trained_models/reg-model-01/export/temp-b'1510688109'/saved_model.pbtxt\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'trained_models/reg-model-01/export/1510688109'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_dir = model_dir + \"/export\"\n",
    "\n",
    "estimator.export_savedmodel(\n",
    "    export_dir_base = export_dir,\n",
    "    serving_input_receiver_fn = csv_serving_input_fn,\n",
    "    as_text=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Serve the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/reg-model-01/export/1510688109\n",
      "INFO:tensorflow:Restoring parameters from b'trained_models/reg-model-01/export/1510688109/variables/variables'\n",
      "{'predictions': array([[ 13.15929985],\n",
      "       [-13.96904373]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "saved_model_dir = export_dir + \"/\" + os.listdir(path=export_dir)[-1] \n",
    "\n",
    "print(saved_model_dir)\n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "    export_dir = saved_model_dir,\n",
    "    signature_def_key=\"predict\"\n",
    ")\n",
    "\n",
    "output = predictor_fn({'csv_rows': [\"0.5,1,ax01,bx02\", \"-0.5,-1,ax02,bx02\"]})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we improve?\n",
    "\n",
    "* **Use data files instead of  DataFrames** - pandas dataframes need to fit in memory, and hard to distribute. Working with (sharded) training data files allows reading records in batches (so we can work with large data set regardless the memory size), as well as supporting distributed training (data parallelism).\n",
    "\n",
    "\n",
    "* **Use Experiment APIs** - Experiment API knows how to invoke training and eval loops in a sensible fashion for local & distributed training.\n",
    "\n",
    "\n",
    "* ** Early Stopping** - Use the validation set evaluation to stop the training and avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
