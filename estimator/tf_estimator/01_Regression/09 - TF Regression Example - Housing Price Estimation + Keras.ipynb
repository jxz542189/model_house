{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Price Estimation + Keras\n",
    "\n",
    "This notebook create a house price estimator using tf.Keras and tf.Estimators. The database uses is the UCLA Housing dataset avaiable at [https://www.kaggle.com/apratim87/housingdata/data](https://www.kaggle.com/apratim87/housingdata/data).\n",
    "\n",
    "\n",
    "## Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import math\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'housing-price-model-01'\n",
    "\n",
    "DATA_FILE = 'data/housingdata.csv'\n",
    "\n",
    "TRAIN_DATA_FILES_PATTERN = 'data/housing-train-01.csv'\n",
    "TEST_DATA_FILES_PATTERN = 'data/housing-test-01.csv'\n",
    "\n",
    "RESUME_TRAINING = False\n",
    "PROCESS_FEATURES = True\n",
    "MULTI_THREADING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "HEADER_DEFAULTS = [[0.0],[0.0],[0.0],['NA'],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0]]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['CRIM', 'ZN','INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY = {'CHAS':['0', '1']}\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "TARGET_NAME = 'MEDV'\n",
    "\n",
    "UNUSED_FEATURE_NAMES = list(set(HEADER) - set(FEATURE_NAMES) - {TARGET_NAME})\n",
    "\n",
    "print(\"Header: {}\".format(HEADER))\n",
    "print(\"Numeric Features: {}\".format(NUMERIC_FEATURE_NAMES))\n",
    "print(\"Categorical Features: {}\".format(CATEGORICAL_FEATURE_NAMES))\n",
    "print(\"Target: {}\".format(TARGET_NAME))\n",
    "print(\"Unused Features: {}\".format(UNUSED_FEATURE_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Analyse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_dataset = pd.read_csv(DATA_FILE, header=None, names=HEADER )\n",
    "housing_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(housing_dataset)\n",
    "\n",
    "print(\"Dataset size: {}\".format(DATA_SIZE))\n",
    "\n",
    "train_data = housing_dataset.sample(frac=0.70, random_state = 19830610)\n",
    "test_data = housing_dataset[~housing_dataset.index.isin(train_data.index)]\n",
    "\n",
    "TRAIN_DATA_SIZE = len(train_data)\n",
    "TEST_DATA_SIZE = len(test_data)\n",
    "\n",
    "print(\"Train set size: {}\".format(TRAIN_DATA_SIZE))\n",
    "print(\"Test set size: {}\".format(TEST_DATA_SIZE))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(path_or_buf=\"data/housing-train-01.csv\", header=False, index=False)\n",
    "test_data.to_csv(path_or_buf=\"data/housing-test-01.csv\", header=False, index=False)\n",
    "\n",
    "pd.read_csv(\"data/housing-train-01.csv\", header=None, names=HEADER).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Input Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Parsing and preprocessing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_row(csv_row):\n",
    "    \n",
    "    columns = tf.decode_csv(csv_row, record_defaults=HEADER_DEFAULTS)\n",
    "    features = dict(zip(HEADER, columns))\n",
    "    \n",
    "    for column in UNUSED_FEATURE_NAMES:\n",
    "        features.pop(column)\n",
    "    \n",
    "    target = features.pop(TARGET_NAME)\n",
    "\n",
    "    return features, target\n",
    "\n",
    "\n",
    "def process_features(features):\n",
    "    \n",
    "    features['CRIM'] = tf.log(features['CRIM']+0.01)\n",
    "    features['B'] = tf.clip_by_value(features['B'], clip_value_min=300, clip_value_max=500)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Data pipeline input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_input_fn(files_name_pattern, mode=tf.estimator.ModeKeys.EVAL, \n",
    "                 skip_header_lines=0, \n",
    "                 num_epochs=None, \n",
    "                 batch_size=200):\n",
    "    \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    num_threads = multiprocessing.cpu_count() if MULTI_THREADING else 1\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"* data input_fn:\")\n",
    "    print(\"================\")\n",
    "    print(\"Input file(s): {}\".format(files_name_pattern))\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    print(\"Epoch Count: {}\".format(num_epochs))\n",
    "    print(\"Mode: {}\".format(mode))\n",
    "    print(\"Thread Count: {}\".format(num_threads))\n",
    "    print(\"Shuffle: {}\".format(shuffle))\n",
    "    print(\"================\")\n",
    "    print(\"\")\n",
    "    \n",
    "    file_names = tf.matching_files(files_name_pattern)\n",
    "\n",
    "    dataset = data.TextLineDataset(filenames=file_names)\n",
    "    dataset = dataset.skip(skip_header_lines)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda csv_row: parse_csv_row(csv_row),  num_parallel_calls=num_threads)\n",
    "    \n",
    "    if PROCESS_FEATURES:\n",
    "        dataset = dataset.map(lambda features, target: (process_features(features), target),  \n",
    "                              num_parallel_calls=num_threads)\n",
    "    \n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    features, target = iterator.get_next()\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = csv_input_fn(files_name_pattern=\"\")\n",
    "print(\"Features in CSV: {}\".format(list(features.keys())))\n",
    "print(\"Target in CSV: {}\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feature Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Create Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns(hparams):\n",
    "    \n",
    "    numeric_columns = [\n",
    "        tf.feature_column.numeric_column(feature_name) for feature_name in NUMERIC_FEATURE_NAMES]\n",
    "    \n",
    "\n",
    "\n",
    "    indicator_columns = [\n",
    "        tf.feature_column.indicator_column(\n",
    "           tf.feature_column.categorical_column_with_vocabulary_list(item[0], item[1]))\n",
    "         for item in CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.items()]\n",
    "        \n",
    "    return numeric_columns + indicator_columns\n",
    "\n",
    "feature_columns = get_feature_columns(tf.contrib.training.HParams(num_buckets=5,embedding_size=3))\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Custom Estimator with Keras\n",
    "\n",
    "We are going to define our custom estimator using ```tf.keras.Model```, ```tf.contrib.estimator.regression_head```, and ```tf.estimator.Estimator```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Define model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params, config):\n",
    "    feature_columns = get_feature_columns(params)\n",
    "    # Create first \"numerical\" layer by concatenating the features\n",
    "    # transformed according to features_columns\n",
    "    input_layer = feature_column.input_layer(features, feature_columns)\n",
    "    \n",
    "    # Defining the tf.keras.Model\n",
    "    \n",
    "    # The first step is defining a tf.keras.Input that matches\n",
    "    # the dimension on input_layer\n",
    "    input_layer_dimension = input_layer.shape.as_list()[1]\n",
    "    inputs = tf.keras.Input(shape=(input_layer_dimension, ))\n",
    "    # The second step is defining the hidden layers\n",
    "    x = tf.keras.layers.Dense(params.hidden_units[0], activation=tf.nn.relu)(inputs)\n",
    "    for layer_size in params.hidden_units[1:]:\n",
    "        x = tf.keras.layers.Dense(layer_size, activation=tf.nn.relu)(x)\n",
    "      \n",
    "    # The output layer has size 1 because we are solving a regression problem\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "    # The final step is defining the tf.keras.Model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Now that we have our model we can compute the value of the logits \n",
    "    logits = model(input_layer)\n",
    "    \n",
    "    \n",
    "    def _train_op_fn(loss):\n",
    "        \"\"\"Returns the op to optimize the loss.\"\"\"\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "        # Create training operation\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        return train_op\n",
    "    \n",
    "    \n",
    "    head = tf.contrib.estimator.regression_head(\n",
    "            label_dimension=1,\n",
    "            name='regression_head'\n",
    "        )\n",
    "    \n",
    "    return head.create_estimator_spec(\n",
    "            features,\n",
    "            mode,\n",
    "            logits,\n",
    "            labels=labels,\n",
    "            train_op_fn=_train_op_fn\n",
    "        )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Define new metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(labels, predictions):\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    pred_values = predictions['predictions']\n",
    "\n",
    "    metrics[\"mae\"] = tf.metrics.mean_absolute_error(labels, pred_values)\n",
    "    metrics[\"rmse\"] = tf.metrics.root_mean_squared_error(labels, pred_values)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Define the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(run_config, hparams):\n",
    "    \n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn, \n",
    "        config=run_config,\n",
    "        params=hparams\n",
    "    )\n",
    "    \n",
    "    \n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, metric_fn)\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Set HParam and RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = TRAIN_DATA_SIZE\n",
    "NUM_EPOCHS = 10000\n",
    "BATCH_SIZE = 177\n",
    "EVAL_AFTER_SEC = 30\n",
    "TOTAL_STEPS = (TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS\n",
    "\n",
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    hidden_units=[16, 8, 4],\n",
    "    max_steps = TOTAL_STEPS\n",
    ")\n",
    "\n",
    "model_dir = 'trained_models/{}'.format(MODEL_NAME)\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    log_step_count_steps=1000,\n",
    "    tf_random_seed=19830610,\n",
    "    model_dir=model_dir\n",
    ")\n",
    "\n",
    "print(hparams)\n",
    "print(\"Model Directory:\", run_config.model_dir)\n",
    "print(\"\")\n",
    "print(\"Dataset Size:\", TRAIN_SIZE)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Steps per Epoch:\",TRAIN_SIZE/BATCH_SIZE)\n",
    "print(\"Total Steps:\", TOTAL_STEPS)\n",
    "print(\"That is 1 evaluation step after each\",EVAL_AFTER_SEC,\" training seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Define TrainSpec and EvaluSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn = lambda: csv_input_fn(\n",
    "        TRAIN_DATA_FILES_PATTERN,\n",
    "        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "        num_epochs=hparams.num_epochs,\n",
    "        batch_size=hparams.batch_size\n",
    "    ),\n",
    "    max_steps=hparams.max_steps,\n",
    "    hooks=None\n",
    ")\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = lambda: csv_input_fn(\n",
    "        TRAIN_DATA_FILES_PATTERN,\n",
    "        mode=tf.estimator.ModeKeys.EVAL,\n",
    "        num_epochs=1,\n",
    "        batch_size=hparams.batch_size,\n",
    "            \n",
    "    ),\n",
    "    throttle_secs = EVAL_AFTER_SEC,\n",
    "    steps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Run Experiment via train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RESUME_TRAINING:\n",
    "    print(\"Removing previous artifacts...\")\n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "else:\n",
    "    print(\"Resuming training...\") \n",
    "\n",
    "    \n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow() \n",
    "print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\") \n",
    "\n",
    "estimator = create_estimator(run_config, hparams)\n",
    "\n",
    "tf.estimator.train_and_evaluate(\n",
    "    estimator=estimator,\n",
    "    train_spec=train_spec, \n",
    "    eval_spec=eval_spec\n",
    ")\n",
    "\n",
    "time_end = datetime.utcnow() \n",
    "print(\".......................................\")\n",
    "print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = lambda: csv_input_fn(files_name_pattern= TRAIN_DATA_FILES_PATTERN, \n",
    "                                      mode= tf.estimator.ModeKeys.EVAL,\n",
    "                                      batch_size= TRAIN_DATA_SIZE)\n",
    "\n",
    "\n",
    "test_input_fn = lambda: csv_input_fn(files_name_pattern= TEST_DATA_FILES_PATTERN, \n",
    "                                      mode= tf.estimator.ModeKeys.EVAL,\n",
    "                                      batch_size= TEST_DATA_SIZE)\n",
    "\n",
    "estimator = create_estimator(run_config, hparams)\n",
    "\n",
    "train_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n",
    "train_rmse = round(math.sqrt(train_results[\"rmse\"]),5)\n",
    "print()\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Train RMSE: {} - {}\".format(train_rmse, train_results))\n",
    "print(\"############################################################################################\")\n",
    "\n",
    "test_results = estimator.evaluate(input_fn=test_input_fn, steps=1)\n",
    "test_rmse = round(math.sqrt(test_results[\"rmse\"]),5)\n",
    "print()\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Test RMSE: {} - {}\".format(test_rmse, test_results))\n",
    "print(\"############################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "predict_input_fn = lambda: csv_input_fn(files_name_pattern= TEST_DATA_FILES_PATTERN, \n",
    "                                      mode= tf.estimator.ModeKeys.PREDICT,\n",
    "                                      batch_size= 5)\n",
    "\n",
    "predictions = estimator.predict(input_fn=predict_input_fn)\n",
    "values = list(map(lambda item: item[\"predictions\"][0],list(itertools.islice(predictions, 5))))\n",
    "print()\n",
    "print(\"Predicted Values: {}\".format(values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
